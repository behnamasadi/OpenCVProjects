{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c9d35b-987b-4d28-9925-24d93e76fa63",
   "metadata": {},
   "source": [
    "# Triangulation\n",
    "triangulation is the process of determining a point in 3D space given its projections onto two, or more, images. It is sometimes also referred to as reconstruction or intersection.\n",
    "In theory if we have the camera matrices, we can compute the intersection camera ray and find the corresponding point in 3D.\n",
    "\n",
    "<img src=\"images/TriangulationIdeal.svg\" />  \n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<img src=\"https://latex.codecogs.com/svg.image?ray_1=\\mathbf{C}^{-1}_1{_{3\\times3}}&space;\\times&space;\\begin{bmatrix}y_1{_u}&space;\\\\y_1_{v}&space;\\\\1\\end{bmatrix}=\\mathbf{C}^{-1}_1\\mathbf{y}_1\" title=\"https://latex.codecogs.com/svg.image?ray_1=\\mathbf{C}^{-1}_1{_{3\\times3}} \\times \\begin{bmatrix}y_1{_u} \\\\y_1_{v} \\\\1\\end{bmatrix}=\\mathbf{C}^{-1}_1\\mathbf{y}_1\" />\n",
    "<br/>\n",
    "<br/>\n",
    "<img src=\"https://latex.codecogs.com/svg.image?ray_2=\\mathbf{C}^{-1}_2{_{3\\times3}}&space;\\times&space;\\begin{bmatrix}y_2{_u}&space;\\\\y_2_{v}&space;\\\\1\\end{bmatrix}=\\mathbf{C}^{-1}_2\\mathbf{y}_2\" title=\"https://latex.codecogs.com/svg.image?ray_2=\\mathbf{C}^{-1}_2{_{3\\times3}} \\times \\begin{bmatrix}y_2{_u} \\\\y_2_{v} \\\\1\\end{bmatrix}=\\mathbf{C}^{-1}_2\\mathbf{y}_2\" />\n",
    "\n",
    "\n",
    "\n",
    "Due to various types of noise (geometric noise from lens distortion or interest point detection error) the lines generated by the corresponding image points do not always intersect in 3D space. The problem, then, is to find a 3D point which optimally fits the measured image points.\n",
    "\n",
    "\n",
    "\n",
    "In the following image, the image points <img src=\"https://latex.codecogs.com/svg.image?y_1\" title=\"https://latex.codecogs.com/svg.image?y_1\" /> and <img src=\"https://latex.codecogs.com/svg.image?y_2\" title=\"https://latex.codecogs.com/svg.image?y_2\" /> are the actual projection of point x but the due to the noise  points <img src=\"https://latex.codecogs.com/svg.image?y'_1\" title=\"https://latex.codecogs.com/svg.image?y'_1\" /> and <img src=\"https://latex.codecogs.com/svg.image?y'_2\" title=\"https://latex.codecogs.com/svg.image?y'_2\" /> are detected on cameras and used for the triangulation. The corresponding projection lines (blue) do not, in general, intersect in 3D space and may also not intersect with point x.\n",
    "\n",
    "\n",
    "<img src=\"images/TriangulationReal.svg\" />\n",
    "\n",
    "\n",
    "\n",
    "A triangulation method can be described in terms of a function  <img src=\"https://latex.codecogs.com/svg.image?\\tau&space;\\\" title=\"https://latex.codecogs.com/svg.image?\\tau \\\" />,  such that\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.image?{\\mathbf&space;&space;{x}}\\sim&space;\\tau&space;({\\mathbf&space;&space;{y}}'_{{1}},{\\mathbf&space;&space;{y}}'_{{2}},{\\mathbf&space;&space;{C}}_{{1}},{\\mathbf&space;&space;{C}}_{{2}})\" title=\"https://latex.codecogs.com/svg.image?{\\mathbf {x}}\\sim \\tau ({\\mathbf {y}}'_{{1}},{\\mathbf {y}}'_{{2}},{\\mathbf {C}}_{{1}},{\\mathbf {C}}_{{2}})\" />\n",
    "\n",
    "\n",
    "\n",
    "where <img src=\"https://latex.codecogs.com/svg.image?&space;{\\mathbf&space;&space;{y}}'_{{1}},{\\mathbf&space;&space;{y}}'_{{2}}\" title=\"https://latex.codecogs.com/svg.image? {\\mathbf {y}}'_{{1}},{\\mathbf {y}}'_{{2}}\" /> are the homogeneous coordinates of the detected image points\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f79cac-203e-4613-86bb-e27d6bb3374d",
   "metadata": {},
   "source": [
    "Triangulation in computer vision refers to determining the 3D position of a point given its projections on two or more images with known camera poses. The PDF you referenced describes a method to solve the triangulation problem using the **Direct Linear Transformation (DLT)** method. Here's an explanation and the equations involved.\n",
    "\n",
    "### **Key Idea**\n",
    "Given two or more camera views, the triangulated 3D point $\\mathbf{X} = [X, Y, Z, 1]^T $ must satisfy the projection equations for all the cameras. Each camera has a projection matrix $\\mathbf{P} $, and the 3D point maps to a 2D image point $\\mathbf{x} = [x, y, 1]^T $. The relationship is:\n",
    "\n",
    "$\n",
    "\\mathbf{x} \\sim \\mathbf{P} \\mathbf{X}\n",
    "$\n",
    "\n",
    "where $\\sim $ means equality up to a scale factor.\n",
    "\n",
    "For a single camera, this expands to:\n",
    "$\n",
    "s \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} = \\mathbf{P} \\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "However, we solve for $\\mathbf{X} $ using at least two views to make the problem well-posed.\n",
    "\n",
    "---\n",
    "\n",
    "### **Projection Matrix and Equations**\n",
    "For two views, we have projection matrices $\\mathbf{P}_1 $ and $\\mathbf{P}_2 $, and corresponding image points $\\mathbf{x}_1 = [x_1, y_1, 1]^T $ and $\\mathbf{x}_2 = [x_2, y_2, 1]^T $.\n",
    "\n",
    "For each view, the equation $\\mathbf{x} \\sim \\mathbf{P} \\mathbf{X} $ leads to two independent constraints (one for $x $, one for $y $):\n",
    "\n",
    "1. For the first view:\n",
    "   $\n",
    "   x_1 (\\mathbf{P}_1^3 \\cdot \\mathbf{X}) - (\\mathbf{P}_1^1 \\cdot \\mathbf{X}) = 0\n",
    "   $\n",
    "   $\n",
    "   y_1 (\\mathbf{P}_1^3 \\cdot \\mathbf{X}) - (\\mathbf{P}_1^2 \\cdot \\mathbf{X}) = 0\n",
    "   $\n",
    "\n",
    "2. For the second view:\n",
    "   $\n",
    "   x_2 (\\mathbf{P}_2^3 \\cdot \\mathbf{X}) - (\\mathbf{P}_2^1 \\cdot \\mathbf{X}) = 0\n",
    "   $\n",
    "   $\n",
    "   y_2 (\\mathbf{P}_2^3 \\cdot \\mathbf{X}) - (\\mathbf{P}_2^2 \\cdot \\mathbf{X}) = 0\n",
    "   $\n",
    "\n",
    "Here, $\\mathbf{P}_i^j $ represents the $j $-th row of the projection matrix $\\mathbf{P}_i $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Matrix Formulation**\n",
    "These equations can be written in matrix form $\\mathbf{A} \\mathbf{X} = 0 $, where $\\mathbf{A} $ is constructed from the projection matrices and image coordinates. For two views:\n",
    "\n",
    "$\n",
    "\\mathbf{A} =\n",
    "\\begin{bmatrix}\n",
    "x_1 \\mathbf{P}_1^3 - \\mathbf{P}_1^1 \\\\\n",
    "y_1 \\mathbf{P}_1^3 - \\mathbf{P}_1^2 \\\\\n",
    "x_2 \\mathbf{P}_2^3 - \\mathbf{P}_2^1 \\\\\n",
    "y_2 \\mathbf{P}_2^3 - \\mathbf{P}_2^2\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "The solution $\\mathbf{X} $ is found by minimizing $\\| \\mathbf{A} \\mathbf{X} \\| $ under the constraint $\\| \\mathbf{X} \\| = 1 $. This is achieved using Singular Value Decomposition (SVD).\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Solve with DLT**\n",
    "1. Construct the matrix $\\mathbf{A} $ from the two projection matrices $\\mathbf{P}_1, \\mathbf{P}_2 $ and image points $\\mathbf{x}_1, \\mathbf{x}_2 $.\n",
    "2. Perform SVD on $\\mathbf{A} $: $\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T $.\n",
    "3. The solution $\\mathbf{X} $ is the last column of $\\mathbf{V} $, corresponding to the smallest singular value of $\\mathbf{A} $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Geometric Interpretation**\n",
    "Triangulation geometrically finds the 3D point $\\mathbf{X} $ that minimizes the reprojection error, i.e., the discrepancy between the observed 2D points and the projections of $\\mathbf{X} $.\n",
    "\n",
    "The DLT method is linear and efficient, but it assumes perfect knowledge of camera parameters and does not inherently minimize reprojection error. Refinements such as iterative optimization (e.g., Levenberg-Marquardt) can improve accuracy.\n",
    "\n",
    "Would you like help with an example or further clarifications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aea63b-77d5-43f6-8710-c06cd267ff6a",
   "metadata": {},
   "source": [
    "```cpp\n",
    "cv::triangulatePoints(InputArray projMatr1, InputArray projMatr2,\n",
    "                    InputArray projPoints1, InputArray projPoints2,\n",
    "                    OutputArray points4D)\t\t\n",
    "```\n",
    "\n",
    "[C++ code](../src/triangulation.cpp)\n",
    "\n",
    "\n",
    "- **What does `cv::recoverPose()` return as $R_1^2$ and $\\mathbf{t_1^2}$?**  \n",
    " The returned rotation $R_1^2$ and translation $\\mathbf{t_1^2}$ transform 3D points **from the first camera's coordinate system** into the second camera's coordinate system. Formally,\n",
    "  $\n",
    "    \\mathbf{X}_{\\text{(cam2)}} \\;=\\; R_1^2 \\,\\mathbf{X}_{\\text{(cam1)}} \\;+\\; \\mathbf{t_1^2}.\n",
    "  $\n",
    "\n",
    "- **How do we set up the projection matrices for triangulation?**  \n",
    "  Typically, we choose the **first camera** as the world/reference frame. Therefore the first camera’s extrinsic is the identity:\n",
    "  $\n",
    "    P_1 \\;=\\; [\\,I \\mid \\mathbf{0}\\,],\n",
    "  $  \n",
    "  and the second camera’s extrinsic is\n",
    "  $\n",
    "    P_2 \\;=\\; [\\,R \\mid \\mathbf{t}\\,].\n",
    "  $\n",
    "\n",
    "- **Which camera is at the origin vs. which camera is “moved”?**  \n",
    "  If you say “Camera 1 is at the origin,” that implicitly means Camera 1’s extrinsic matrix is $[I \\mid \\mathbf{0}]$. Then Camera 2 is “moved” by $[R \\mid \\mathbf{t}]$.  \n",
    "\n",
    "Putting it all together for triangulation in OpenCV:\n",
    "\n",
    "1. After finding the essential matrix $\\mathbf{E}$ and calling\n",
    "   ```cpp\n",
    "   cv::Mat R, t, mask;\n",
    "   cv::recoverPose(E, points1, points2, K, R, t, mask);\n",
    "   ```\n",
    "   we get $R, t$ that map from Camera 1 to Camera 2.\n",
    "\n",
    "2. We define\n",
    "   ```cpp\n",
    "   cv::Mat P1 = cv::Mat::eye(3, 4, CV_64F);  // [I | 0]\n",
    "   cv::Mat P2(3, 4, CV_64F);                 // [R | t]\n",
    "   R.copyTo(P2(cv::Rect(0, 0, 3, 3)));\n",
    "   t.copyTo(P2(cv::Rect(3, 0, 1, 3)));\n",
    "   ```\n",
    "   Then multiply by $K$:\n",
    "   ```cpp\n",
    "   cv::Mat KP1 = K * P1;\n",
    "   cv::Mat KP2 = K * P2;\n",
    "   ```\n",
    "\n",
    "3. Finally, call\n",
    "   ```cpp\n",
    "   cv::triangulatePoints(KP1, KP2, points1, points2, points4D);\n",
    "   ```\n",
    "   where `points1` are the 2D points in the first image, `points2` are the matched 2D points in the second image.\n",
    "\n",
    "---\n",
    "\n",
    "In the code\n",
    "\n",
    "```cpp\n",
    "cv::Mat R, t, mask;\n",
    "cv::recoverPose(E, points1, points2, K, R, t, mask);\n",
    "\n",
    "// P1 = [I|0]\n",
    "cv::Mat P1 = cv::Mat::eye(3, 4, CV_64F);\n",
    "\n",
    "// P2 = [R|t]\n",
    "cv::Mat P2(3, 4, CV_64F);\n",
    "R.copyTo(P2(cv::Rect(0, 0, 3, 3)));\n",
    "t.copyTo(P2(cv::Rect(3, 0, 1, 3)));\n",
    "\n",
    "// Triangulate\n",
    "cv::Mat points4D;\n",
    "cv::triangulatePoints(K * P1, K * P2, points1, points2, points4D);\n",
    "```\n",
    "\n",
    "- You have **Camera 1** as your reference ($[I \\mid \\mathbf{0}]$).  \n",
    "- You have **Camera 2** described by $[R \\mid \\mathbf{t}]$.  \n",
    "- You feed `points1` (from image 1) and `points2` (from image 2) in the same order to `triangulatePoints`.  \n",
    "- You multiply each by `K` once (i.e., `K * P1` and `K * P2`).  \n",
    "\n",
    "This usage **is correct** given that OpenCV’s `recoverPose` indeed gives you the transform from Camera 1 → Camera 2.  \n",
    "\n",
    "### Common Gotcha\n",
    "If you accidentally swapped `points1` and `points2` when calling `recoverPose`, or if you tried to interpret $[R \\mid \\mathbf{t}]$ incorrectly (e.g., placing the second camera as the identity and the first camera as $[R \\mid \\mathbf{t}]$ without inverting), then you would get inconsistent 3D reconstructions or “flipped” motion. But as posted, your setup matches the usual convention:\n",
    "\n",
    "- **First camera** = reference = $[I \\mid \\mathbf{0}]$.  \n",
    "- **Second camera** = $[R \\mid \\mathbf{t}]$ from `recoverPose`.  \n",
    "\n",
    "Hence, **your projection matrices and your triangulation call look correct** for the standard pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e53e4d-d6d8-4825-a208-c2fc481fd616",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "\n",
    "Define the object points (corners of a square on the plane Z=0)\n",
    "```python\n",
    "\n",
    "object_points = np.array([\n",
    "    [0, 0, 0],  # Point 1 (origin)\n",
    "    [1, 0, 0],  # Point 2 (1 unit along the X-axis)\n",
    "    [1, 1, 0],  # Point 3 (1 unit up along the Y-axis)\n",
    "    [0, 1, 0]   # Point 4 (back to Y-axis)\n",
    "], dtype=np.float32)\n",
    "```\n",
    "\n",
    "Camera intrinsic parameters (assuming a simple pinhole camera model)\n",
    "```python\n",
    "focal_length = 1.0  # Example focal length\n",
    "principal_point = (0.5, 0.5)  # Assuming the principal point is at the center\n",
    "\n",
    "camera_matrix = np.array([\n",
    "    [focal_length, 0, principal_point[0]],\n",
    "    [0, focal_length, principal_point[1]],\n",
    "    [0, 0, 1]\n",
    "], dtype=np.float32)\n",
    "```\n",
    "Camera 1 extrinsic parameters (position and orientation), assume camera 1 is at origin looking along the Z-axis\n",
    "```python\n",
    "rotation_vector_1 = np.array([0, 0, 0], dtype=np.float32)  # No rotation\n",
    "translation_vector_1 = np.array([0, 0, 1], dtype=np.float32)  # 1 unit away from the origin along Z-axis\n",
    "```\n",
    "\n",
    "Camera 2 extrinsic parameters (position and orientation), assume camera 2 is positioned differently\n",
    "\n",
    "```python\n",
    "rotation_vector_2 = np.array([0, np.deg2rad(45), 0], dtype=np.float32)  # Rotated 45 degrees around Y-axis\n",
    "translation_vector_2 = np.array([1, 0, 0.5], dtype=np.float32)  # Different position\n",
    "```\n",
    "\n",
    "Project points onto each camera's image plane, The `rotation_vector_1`, `translation_vector_1` and `rotation_vector_2`, `translation_vector_2` describe the transformation of the **world in the camera's coordinate system**.\n",
    "\n",
    "```python\n",
    "rotation_matrix_1, _ = cv2.Rodrigues(rotation_vector_1)\n",
    "rotation_matrix_2, _ = cv2.Rodrigues(rotation_vector_2)\n",
    "\n",
    "projection_matrix_1 = np.hstack((rotation_matrix_1, translation_vector_1.reshape(-1, 1)))\n",
    "projection_matrix_2 = np.hstack((rotation_matrix_2, translation_vector_2.reshape(-1, 1)))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Project points using camera matrices (The rotation vector (Rodrigues) that, together with translation vector, performs a change of basis from world to camera coordinate system)\n",
    "\n",
    "```python\n",
    "points_2d_camera_1 = cv2.projectPoints(object_points, rotation_vector_1, translation_vector_1, camera_matrix, None)[0]\n",
    "points_2d_camera_2 = cv2.projectPoints(object_points, rotation_vector_2, translation_vector_2, camera_matrix, None)[0]\n",
    "points_2d_camera_1 = points_2d_camera_1.reshape(-1, 2) # Shape (2, N)\n",
    "points_2d_camera_2 = points_2d_camera_2.reshape(-1, 2) # Shape (2, N)\n",
    "```\n",
    "\n",
    "\n",
    "`points_2d_camera_1`:\n",
    "```\n",
    " [[[0.5 0.5]]\n",
    "\n",
    " [[1.5 0.5]]\n",
    "\n",
    " [[1.5 1.5]]\n",
    "\n",
    " [[0.5 1.5]]]\n",
    "```\n",
    "\n",
    " \n",
    "`points_2d_camera_2`:\n",
    "```\n",
    " [[ 2.5       0.5     ]\n",
    " [-7.74264   0.5     ]\n",
    " [-7.74264  -4.328427]\n",
    " [ 2.5       2.5     ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1543dc7-55f7-49c8-8934-4d8e72fd12fd",
   "metadata": {},
   "source": [
    "```\n",
    "cv.triangulatePoints(\tprojMatr1, projMatr2, projPoints1, projPoints2[, points4D]\t) ->\tpoints4D\n",
    "```\n",
    "\n",
    "\n",
    "- `projMatr1`\t3x4 projection matrix of the first camera, i.e. this matrix projects 3D points given in the world's coordinate system into the first image.\n",
    "- `projMatr2`\t3x4 projection matrix of the second camera, i.e. this matrix projects 3D points given in the world's coordinate system into the second image.\n",
    "- `projPoints1`\t2xN array of feature points in the first image. In the case of the c++ version, it can be also a vector of feature points or two-channel matrix of size 1xN or Nx1.\n",
    "- `projPoints2`\t2xN array of corresponding points in the second image. In the case of the c++ version, it can be also a vector of feature points or two-channel matrix of size 1xN or Nx1.\n",
    "- `points4D`\t4xN array of reconstructed points in homogeneous coordinates. **These points are returned in the world's coordinate system**\n",
    "\n",
    "  Ref: [1](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#gad3fc9a0c82b08df034234979960b778c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb92915-2fb6-4dce-8b81-5915caf1258d",
   "metadata": {},
   "source": [
    "Perform triangulation\n",
    "\n",
    "```\n",
    "homogeneous_3d_points = cv2.triangulatePoints(\n",
    "    full_projection_matrix_1, full_projection_matrix_2,\n",
    "    points_2d_camera_1, points_2d_camera_2\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f2e817-518d-4406-aca9-09675c0ea4c7",
   "metadata": {},
   "source": [
    "Convert from homogeneous coordinates to 3D\n",
    "\n",
    "```\n",
    "triangulated_3d_points = homogeneous_3d_points[:3] / homogeneous_3d_points[3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4048fc93-dde0-452e-b7ab-cecfcbd6206f",
   "metadata": {},
   "source": [
    "Triangulated 3D points:\n",
    "\n",
    "```\n",
    " [[ 0.          0.          0.        ]\n",
    " [ 0.9999998   0.         -0.00000016]\n",
    " [ 0.9999999   0.9999999  -0.00000014]\n",
    " [ 0.          1.         -0.        ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2fa305-8c5d-4350-bd75-e9e2901663d9",
   "metadata": {},
   "source": [
    "\n",
    "The triangulation process successfully reconstructed the 3D positions of the points. The reconstructed 3D points are:\n",
    "\n",
    "- Point 1: <img src=\"https://latex.codecogs.com/svg.image?\\([0,%200,%200]\\)\" title=\"https://latex.codecogs.com/svg.image?\\([0, 0, 0]\\)\" />\n",
    "- Point 2: <img src=\"https://latex.codecogs.com/svg.image?\\([1,%200,%200]\\)\" title=\"https://latex.codecogs.com/svg.image?\\([1, 0, 0]\\)\" />\n",
    "- Point 3: <img src=\"https://latex.codecogs.com/svg.image?\\([1,%201,%200]\\)\" title=\"https://latex.codecogs.com/svg.image?\\([1, 1, 0]\\)\" />\n",
    "- Point 4: <img src=\"https://latex.codecogs.com/svg.image?\\([0,%201,%200]\\)\" title=\"https://latex.codecogs.com/svg.image?\\([0, 1, 0]\\)\" />\n",
    "\n",
    "These reconstructed points closely match the original 3D coordinates of the square's corners. The slight differences are due to numerical precision and the inherent limitations of the triangulation process.\n",
    "\n",
    "This demonstrates that the process of projecting 3D points into 2D camera images and then reconstructing them back into 3D space using triangulation is effective. The reconstructed points can be used to determine the absolute pose of the object in the world coordinate system.\n",
    "\n",
    "\n",
    "[Python code](../scripts/triangulation.py)\n",
    "\n",
    "Refs: [1](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#gad3fc9a0c82b08df034234979960b778c), [2](https://en.wikipedia.org/wiki/Triangulation_(computer_vision)), [3](https://www.cs.cmu.edu/~16385/s17/Slides/11.4_Triangulation.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
