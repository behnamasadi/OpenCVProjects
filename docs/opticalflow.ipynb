{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "179db1ab-7374-4fa7-aedf-67d23d3d9185",
   "metadata": {},
   "source": [
    "## 1-D Taylor expansion and substitution\n",
    "\n",
    "Taylor expansion of $f$ around $a$:\n",
    "$$\n",
    "f(x)=f(a)+f'(a)(x-a)+\\frac{1}{2},f''(a)(x-a)^2+\\frac{1}{6},f^{(3)}(a)(x-a)^3+\\cdots\n",
    "$$\n",
    "\n",
    "Substitute $x=a+\\Delta x$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(a+\\Delta x)\n",
    "&=f(a)+f'(a),\\Delta x+\\frac{1}{2},f''(a),(\\Delta x)^2+\\frac{1}{6},f^{(3)}(a),(\\Delta x)^3+\\cdots\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413cb005-3dee-4596-9ad5-0f473f6de66c",
   "metadata": {},
   "source": [
    "##  Multivariable (2-D) second-order Taylor polynomial\n",
    "\n",
    "Let $f:\\mathbb{R}^2\\to\\mathbb{R}$, expand around $(a,b)$. Denote the gradient $Df(a,b)*{1\\times 2}$ and Hessian $Hf(a,b)*{2\\times 2}$:\n",
    "\n",
    "$$\\\\\n",
    "f(x) = f(a+\\Delta x) = f(a) + f^{'}(a)(a + \\Delta x-a) + \\frac{f^{''}(a)(a + \\Delta x-a)^2}{2!} + \\text{...}\n",
    "\\\\\n",
    "= f(a) +f^{'}(a)\\Delta x+ \\frac{f^{''}(a)}{2!}(\\Delta x)^2 + \\text{...}$$\n",
    "\n",
    "\n",
    "<!-- $$ \\begin{aligned}\n",
    "p_2(x,y)\n",
    "&= f(a,b)\n",
    "* Df(a,b)*{1\\times 2}\n",
    "  \\begin{bmatrix}\n",
    "  x-a[2pt] y-b\n",
    "  \\end{bmatrix}*{2\\times 1}\n",
    "* \\frac{1}{2}\n",
    "  \\begin{bmatrix}\n",
    "  x-a & y-b\n",
    "  \\end{bmatrix}*{1\\times 2}\n",
    "  Hf(a,b)*{2\\times 2}\n",
    "  \\begin{bmatrix}\n",
    "  x-a[2pt] y-b\n",
    "  \\end{bmatrix}_{2\\times 1}.\n",
    "  \\end{aligned}  $$ -->\n",
    "\n",
    "Dimensions: gradient is $1\\times 2$, column offset is $2\\times 1$, Hessian is $2\\times 2$; the quadratic form yields a scalar $1\\times 1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53acd9df-c4e5-4e2d-ae88-6b70f8b4f842",
   "metadata": {},
   "source": [
    "##  Optical flow: brightness constancy and linearization\n",
    "\n",
    "Brightness constancy between times $t$ and $t+\\Delta t$: Between two image frames which are taken at times $t$ and  $t+\\Delta t$ at every position following brightness constancy constraint:\n",
    "\n",
    "$I(x,y,t) = I(x+\\Delta x, y + \\Delta y, t + \\Delta t)$\n",
    "\n",
    "\n",
    "\n",
    "Assuming the movement to be small, Taylor series of:\n",
    "\n",
    "$I(x+\\Delta x, y + \\Delta y, t + \\Delta t)$\n",
    "\n",
    "\n",
    "is:\n",
    "\n",
    "$${\\displaystyle I(x+\\Delta x,y+\\Delta y,t+\\Delta t)=I(x,y,t)+{\\frac {\\partial I}{\\partial x}}\\,\\Delta x+{\\frac {\\partial I}{\\partial y}}\\,\\Delta y+{\\frac {\\partial I}{\\partial t}}\\,\\Delta t+{}} \\text{higher-order terms}$$\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{\\partial I}{\\partial x}\\Delta x+\\frac{\\partial I}{\\partial y}\\Delta y+\\frac{\\partial I}{\\partial t}\\Delta t = 0$$\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "dividing by $\\Delta t$:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{\\partial I}{\\partial x} \\frac{d x}{d t} + \\frac{\\partial I}{\\partial y} \\frac{d y}{d t} + \\frac{\\partial I}{\\partial t} = 0$$\n",
    "\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "$$\\frac{\\partial I}{\\partial x}V_x+\\frac{\\partial I}{\\partial y}V_y+\\frac{\\partial I}{\\partial t} = 0$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n",
    "$$I_xV_x+I_yV_y=-I_t$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$ \\nabla I\\cdot {\\vec {V}}=-I_{t}$\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "**optical flow vector and how to draw it:**\n",
    "In practice the optical flow at pixel $(x,y)$ is $\\vec V(x,y)=(u,v)$ in **pixels per frame** (or pixels/s if time is in seconds). To visualize it, draw an arrow on the **previous** frame from the start point $(x,y)$ to the end point $(x+u,;y+v)$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469b516c-57d3-4af9-b96a-7c286d1f03c5",
   "metadata": {},
   "source": [
    "\n",
    "## Aperture problem (under-constraint)\n",
    "\n",
    "The single linear constraint $$I_x V_x + I_y V_y = -I_t$$\n",
    "\n",
    "has **two unknowns** $(V_x,V_y)$; without extra assumptions there are infinitely many solutions along the isophote tangent direction.\n",
    "\n",
    "##  Lucas–Kanade in a patch (over-determined system)\n",
    "\n",
    "Assume a small window (e.g., $3\\times 3$) where the flow is **constant**: $\\vec V=(V_x,V_y)$ for all $n$ pixels ${q_i}$, where $i=1,2, \\dots, n$ in the local patch. Stack the $n$ equations\n",
    "$$\n",
    "I_x(q_i) V_x + I_y(q_i) V_y = -I_t(q_i),\\qquad i=1,\\dots,n\n",
    "$$\n",
    "into the linear system\n",
    "\n",
    "\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}I_{x}(q_{1})V_{x}+I_{y}(q_{1})V_{y}&=-I_{t}(q_{1})\\\\I_{x}(q_{2})V_{x}+I_{y}(q_{2})V_{y}&=-I_{t}(q_{2})\\\\&\\;\\ \\vdots \\\\I_{x}(q_{n})V_{x}+I_{y}(q_{n})V_{y}&=-I_{t}(q_{n})\\end{aligned}}}$$\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "$$Av=b$$\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "$$\\displaystyle{ A={\\begin{bmatrix}I_{x}(q_{1})&I_{y}(q_{1})\\\\[10pt]I_{x}(q_{2})&I_{y}(q_{2})\\\\[10pt]\\vdots &\\vdots \\\\[10pt]I_{x}(q_{n})&I_{y}(q_{n})\\end{bmatrix}}\\quad \\quad \\quad v={\\begin{bmatrix}V_{x}\\\\[10pt]V_{y}\\end{bmatrix}}\\quad \\quad \\quad b={\\begin{bmatrix}-I_{t}(q_{1})\\\\[10pt]-I_{t}(q_{2})\\\\[10pt]\\vdots \\\\[10pt]-I_{t}(q_{n})\\end{bmatrix}}}$$\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "to solve this problem:\n",
    "\n",
    "$$\\\\\n",
    "{\\displaystyle A^{T}Av=A^{T}b}\\\\\n",
    "\\\\\n",
    "{\\displaystyle \\mathrm {v} =(A^{T}A)^{-1}A^{T}b}$$\n",
    "\n",
    "\n",
    "\n",
    "That is, it computes\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "V_{x} \\\\\n",
    "V_{y}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\sum _{i}I_{x}(q_{i})^{2} & \\sum _{i}I_{x}(q_{i})I_{y}(q_{i}) \\\\\n",
    "\\sum _{i}I_{y}(q_{i})I_{x}(q_{i}) & \\sum _{i}I_{y}(q_{i})^{2}\n",
    "\\end{bmatrix}^{-1}\n",
    "\\begin{bmatrix}\n",
    "-\\sum _{i}I_{x}(q_{i})I_{t}(q_{i}) \\\\\n",
    "-\\sum _{i}I_{y}(q_{i})I_{t}(q_{i})\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "**Invertibility condition (well-posedness):**\n",
    "$(A^\\top A)_{2\\times 2}$ must be well-conditioned (two sufficiently large eigenvalues). Intuitively, the patch should contain **corner-like** texture rather than a flat region (both small eigenvalues) or a pure edge (one small eigenvalue) — this is the classical remedy to the aperture problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55356478-5b45-4646-b673-9214623c641c",
   "metadata": {},
   "source": [
    "## **List of Available Optical Flow Methods**  \n",
    "\n",
    "\n",
    "### **1. Sparse Optical Flow Methods**\n",
    "\n",
    "These estimate motion only for **selected keypoints** (e.g., from Shi-Tomasi or FAST).\n",
    "\n",
    "####  **Lucas–Kanade Optical Flow**\n",
    "\n",
    "* **Function:**\n",
    "  `cv2.calcOpticalFlowPyrLK(prevImg, nextImg, prevPts, ...)`\n",
    "* **Type:** Sparse\n",
    "* **Algorithm:** Classic pyramidal Lucas–Kanade (iterative window-based)\n",
    "* **Typical Use Case:** Tracking corners/keypoints between frames\n",
    "* **Key Parameters:**\n",
    "\n",
    "  * `winSize`, `maxLevel`, `criteria`\n",
    "* **Notes:** Works with grayscale images.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Dense Optical Flow Methods**\n",
    "\n",
    "These estimate motion **for all pixels** in the image.\n",
    "\n",
    "####  **Farnebäck Optical Flow**\n",
    "\n",
    "* **Function:**\n",
    "  `cv2.calcOpticalFlowFarneback(prev, next, flow, pyr_scale, levels, winsize, iterations, poly_n, poly_sigma, flags)`\n",
    "* **Type:** Dense\n",
    "* **Algorithm:** Polynomial expansion (Gunnar Farnebäck, 2003)\n",
    "* **Output:**\n",
    "  $$ \\text{flow} \\in \\mathbb{R}^{H \\times W \\times 2} $$\n",
    "  where flow[...,0]=u (horizontal), flow[...,1]=v (vertical)\n",
    "* **Notes:** Most common dense optical flow in OpenCV.\n",
    "\n",
    "---\n",
    "\n",
    "####  **Dual TV-L1 Optical Flow**\n",
    "\n",
    "* **Class:**\n",
    "\n",
    "  ```python\n",
    "  cv2.optflow.DualTVL1OpticalFlow_create()\n",
    "  ```\n",
    "* **Type:** Dense\n",
    "* **Algorithm:** Total Variation L1 (robust to noise, discontinuities)\n",
    "* **Reference:** Zach et al., “A Duality Based Approach for Realtime TV-L1 Optical Flow”, DAGM 2007\n",
    "* **Notes:** Robust and accurate but slower than Farnebäck.\n",
    "\n",
    "---\n",
    "\n",
    "####  **Brox Optical Flow** (CUDA)\n",
    "\n",
    "* **Class:**\n",
    "  `cv2.cuda_BroxOpticalFlow.create()`\n",
    "* **Type:** Dense (GPU)\n",
    "* **Algorithm:** Variational (Brox et al. 2004)\n",
    "* **Notes:** Requires CUDA; more accurate for smooth motion.\n",
    "\n",
    "---\n",
    "\n",
    "####  **PCAFlow**\n",
    "\n",
    "* **Class:**\n",
    "  `cv2.optflow.createOptFlow_PCAFlow()`\n",
    "* **Type:** Dense\n",
    "* **Algorithm:** Dimensionality reduction of flow fields using PCA\n",
    "* **Reference:** Wulff and Black, CVPR 2015\n",
    "* **Notes:** Efficient; handles large displacements moderately well.\n",
    "\n",
    "---\n",
    "\n",
    "####  **SimpleFlow**\n",
    "\n",
    "* **Class:**\n",
    "  `cv2.optflow.createOptFlow_SimpleFlow()`\n",
    "* **Type:** Dense\n",
    "* **Algorithm:** SimpleFlow (Liu et al. 2010)\n",
    "* **Notes:** Designed for simplicity and robustness; slower than Farnebäck.\n",
    "\n",
    "---\n",
    "\n",
    "####  **DeepFlow**\n",
    "\n",
    "* **Class:**\n",
    "  `cv2.optflow.createOptFlow_DeepFlow()`\n",
    "* **Type:** Dense\n",
    "* **Algorithm:** DeepFlow (Weinzaepfel et al. 2013)\n",
    "* **Notes:** Handles large displacements better than Farnebäck.\n",
    "\n",
    "---\n",
    "\n",
    "#### **DIS Optical Flow (Dense Inverse Search)**\n",
    "\n",
    "* **Class:**\n",
    "  `cv2.optflow.createOptFlow_DIS()`\n",
    "* **Type:** Dense\n",
    "* **Algorithm:** DIS (Kroeger et al. 2016)\n",
    "* **Modes:**\n",
    "\n",
    "  * `cv2.optflow.DISOPTICAL_FLOW_PRESET_ULTRAFAST`\n",
    "  * `cv2.optflow.DISOPTICAL_FLOW_PRESET_FAST`\n",
    "  * `cv2.optflow.DISOPTICAL_FLOW_PRESET_MEDIUM`\n",
    "* **Notes:** Extremely fast; less accurate for large displacements.\n",
    "\n",
    "---\n",
    "\n",
    "####  **RLOF (Robust Local Optical Flow)**\n",
    "\n",
    "* **Class:**\n",
    "  `cv2.optflow.createOptFlow_RLOF()`\n",
    "* **Type:** Dense/Sparse hybrid\n",
    "* **Algorithm:** RLOF (Senst et al. 2016)\n",
    "* **Notes:** Robust against illumination changes; can be combined with sparse tracking (e.g. pyramids).\n",
    "\n",
    "---\n",
    "\n",
    "####  **SparseToDense Optical Flow**\n",
    "\n",
    "* **Class:**\n",
    "  `cv2.optflow.createOptFlow_SparseToDense()`\n",
    "* **Type:** Dense\n",
    "* **Algorithm:** Interpolates sparse flow (from Lucas–Kanade or RLOF) into a dense field.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Deep Learning–Based Methods**\n",
    "\n",
    "These rely on pre-trained deep networks.\n",
    "\n",
    "####  **DeepFlow (classical, not deep learning)**\n",
    "\n",
    "→ already listed above (don’t confuse with FlowNet).\n",
    "\n",
    "####  **DIS Optical Flow with Deep Matching**\n",
    "\n",
    "OpenCV doesn’t ship FlowNet or RAFT, but you can integrate them externally:\n",
    "\n",
    "* **FlowNet2**, **PWC-Net**, **RAFT**, **GMFlow**, etc.\n",
    "  Available via PyTorch or TensorFlow implementations.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. GPU Implementations (CUDA)**\n",
    "\n",
    "OpenCV provides GPU-accelerated versions for some methods:\n",
    "\n",
    "| Algorithm          | Class                                      | Notes                  |\n",
    "| ------------------ | ------------------------------------------ | ---------------------- |\n",
    "| **Brox**           | `cv2.cuda_BroxOpticalFlow.create()`        | variational            |\n",
    "| **Farnebäck**      | `cv2.cuda_FarnebackOpticalFlow.create()`   | same as CPU but faster |\n",
    "| **Dual TV-L1**     | `cv2.cuda_OpticalFlowDual_TVL1.create()`   | robust                 |\n",
    "| **PyrLK (Sparse)** | `cv2.cuda_SparsePyrLKOpticalFlow.create()` | for keypoints          |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "| Method        | Type         | Module                     | GPU Support | Notes               |\n",
    "| ------------- | ------------ | -------------------------- | ----------- | ------------------- |\n",
    "| Lucas–Kanade  | Sparse       | `cv2`                      | ✅ (CUDA)    | Keypoint tracking   |\n",
    "| Farnebäck     | Dense        | `cv2` / `cv2.cuda`         | ✅           | Fast & common       |\n",
    "| Dual TV-L1    | Dense        | `cv2.optflow` / `cv2.cuda` | ✅           | Robust, slower      |\n",
    "| Brox          | Dense        | `cv2.cuda`                 | ✅           | Smooth, variational |\n",
    "| PCAFlow       | Dense        | `cv2.optflow`              | ❌           | PCA-based           |\n",
    "| SimpleFlow    | Dense        | `cv2.optflow`              | ❌           | Easy to use         |\n",
    "| DeepFlow      | Dense        | `cv2.optflow`              | ❌           | Large motion        |\n",
    "| DIS           | Dense        | `cv2.optflow`              | ❌           | Very fast           |\n",
    "| RLOF          | Sparse/Dense | `cv2.optflow`              | ❌           | Robust to lighting  |\n",
    "| SparseToDense | Dense        | `cv2.optflow`              | ❌           | Interpolated flow   |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f7cbb7-22ae-459c-9765-8cfc5cfa394b",
   "metadata": {},
   "source": [
    "# OpenCV OpticalFlow API Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723b2c82-a80b-4b44-926d-9d178589c0f7",
   "metadata": {},
   "source": [
    "When you call:\n",
    "```python\n",
    "flow = cv2.calcOpticalFlowFarneback(\n",
    "    prev=prev,\n",
    "    next=next,\n",
    "    # output flow array (None -> allocate automatically)\n",
    "    flow=None,\n",
    "    pyr_scale=0.5,          # image scale (<1) for pyramid\n",
    "    levels=3,               # number of pyramid levels\n",
    "    winsize=15,             # averaging window size\n",
    "    iterations=3,           # iterations per pyramid level\n",
    "    poly_n=5,               # size of pixel neighborhood\n",
    "    poly_sigma=1.2,         # std of Gaussian for derivatives\n",
    "    flags=0                 # usually 0 or cv2.OPTFLOW_FARNEBACK_GAUSSIAN\n",
    ")\n",
    "```\n",
    "\n",
    "OpenCV computes **dense optical flow from `prev` to `next`**.\n",
    "\n",
    "\n",
    "Where, flow is an array of shape $(H, W, 2)$ :\n",
    "\n",
    "- horizontal displacement $u$\n",
    "- vertical displacement   $v$\n",
    "```python\n",
    "    \n",
    "\n",
    "u= flow[...,0]\n",
    "v= flow[...,1]\n",
    "```\n",
    "\n",
    "\n",
    "That means:\n",
    "\n",
    "For every pixel `(x, y)` in the **previous frame**, `flow[y, x] = (u, v)` tells how far that pixel **moved** between frames.\n",
    "\n",
    "Mathematically: $ (x', y') = (x + u(x,y),\\ y + v(x,y))$\n",
    "\n",
    "So:\n",
    "\n",
    "* `(x, y)` = pixel coordinates in **prev**\n",
    "* `(u, v)` = motion vector\n",
    "* `(x', y')` = new position of that same pixel in **next**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Frame at time `t` (`prev`):\n",
    "\n",
    "```\n",
    "[10, 20, 30]\n",
    "```\n",
    "\n",
    "Frame at time `t+1` (`next`):\n",
    "\n",
    "```\n",
    "[ 0, 10, 20]\n",
    "```\n",
    "\n",
    "```cpp\n",
    "              u (x = column)\n",
    "    ------------------------------------------►\n",
    "    | (0,0) (1,0) (2,0) (3,0)\n",
    "    | (0,1) (1,1) (2,1) (3,1)\n",
    "    | (0,2) (1,2) (2,2) (3,2)\n",
    "  v | \n",
    "    |\n",
    "    ▼\n",
    "      (y = row)\n",
    "```\n",
    "\n",
    "\n",
    "The bright region moved **right by 1 pixel**.\n",
    "\n",
    "So for each valid pixel in the previous frame:\n",
    "\n",
    "| Pixel (x,y) | Motion (u,v) | Means                               |\n",
    "| ----------- | ------------ | ----------------------------------- |\n",
    "| (0,0)       | (1,0)        | pixel moved to (1,0) in next        |\n",
    "| (1,0)       | (1,0)        | pixel moved to (2,0) in next        |\n",
    "| (2,0)       | (1,0)        | pixel moved to (3,0) — out of frame |\n",
    "\n",
    "That’s **forward optical flow**.\n",
    "\n",
    "---\n",
    "\n",
    "###  Meaning of `u` and `v`\n",
    "\n",
    "| Symbol   | Meaning                                | Units  |\n",
    "| -------- | -------------------------------------- | ------ |\n",
    "| `u(x,y)` | horizontal displacement between frames | pixels |\n",
    "| `v(x,y)` | vertical displacement between frames   | pixels |\n",
    "\n",
    "Positive `u` means movement **to the right**.\n",
    "Positive `v` means movement **downward** (since y increases downward in images).\n",
    "\n",
    "So the motion vector **(u,v)** describes a pixel’s 2D velocity (in pixels per frame).\n",
    "\n",
    "---\n",
    "\n",
    "### Visualization Intuition\n",
    "\n",
    "If you visualize optical flow as arrows on the image:\n",
    "\n",
    "```python\n",
    "(x, y)  ->  (x + u, y + v)\n",
    "```\n",
    "\n",
    "The arrow’s direction and length represent where that pixel moved.\n",
    "Farneback estimates these displacements for **every pixel** (dense flow).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6233932a-e054-4536-89a9-c472b0224dda",
   "metadata": {},
   "source": [
    "![](images/4.OpticalFlow_(Dense_Arrows).png)\n",
    "![](images/OpticalFlow_(Dense_HSV).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657a7b3d-7d13-476f-9b83-383b529d2fb6",
   "metadata": {},
   "source": [
    "### **Wrapping** and **Inverse Mapping**\n",
    "Warping is a geometric transformation that uses the optical flow to transform (or \"warp\") one image to match another.\n",
    "\n",
    "\n",
    "**What warping does:**\n",
    "- Takes the `prev` frame and moves each pixel according to the flow vectors (u, v)\n",
    "- If the optical flow is accurate, the warped `prev` frame should look very similar to the `next` frame\n",
    "\n",
    "\n",
    "Pixel at `prev(point(x,y))` has been moved to `next(point(x+u, y+v))`”\n",
    "\n",
    "```\n",
    "prev frame:          next frame:\n",
    "P(x,y) --------------> P'(x+u, y+v)\n",
    "```\n",
    "\n",
    "That’s why to **warp** the previous frame toward the next,\n",
    "you must perform **inverse mapping**:\n",
    "\n",
    "```python\n",
    "map_x = x - u\n",
    "map_y = y - v\n",
    "```\n",
    "\n",
    "Because you want to say:\n",
    "\n",
    "> “For each pixel in the next frame, where do I sample it from in the previous frame?”\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "| Concept                                   | Maps from | To   | Formula                      |\n",
    "| ----------------------------------------- | --------- | ---- | ---------------------------- |\n",
    "| Forward flow (`calcOpticalFlowFarneback`) | prev      | next | $x’,y’$ = $x+u, y+v$         |\n",
    "| Inverse flow (for remap)                  | next      | prev | map_x = x - u, map_y = y - v |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Mathematical concept:**\n",
    "For each pixel at position $(x, y)$ in `prev`:\n",
    "- The flow vector tells us it moved by $(u, v)$\n",
    "- So that pixel should now be at position $(x + u, y + v)$ in `next`\n",
    "- Warping reconstructs what `next` should look like by relocating all pixels from `prev` according to their flow\n",
    "\n",
    "**Why it's useful:**\n",
    "1. **Validation**: Compare warped `prev` with actual `next` - if they match well, the flow is accurate\n",
    "2. **Error measurement**: The difference between warped `prev` and `next` shows where the flow estimation failed\n",
    "3. **Applications**: Frame interpolation, video stabilization, image registration\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "**Key insight:** If the optical flow is perfect, the warped previous frame should look identical to the next frame, and the difference image would be black (zero error). Any bright regions in the difference image indicate areas where:\n",
    "- The flow estimation was inaccurate\n",
    "- There were occlusions (objects appearing/disappearing)\n",
    "- Lighting changed\n",
    "- The motion model doesn't fit (e.g., non-rigid deformations)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "###  Visual Example\n",
    "\n",
    "Let’s visualize with coordinates:\n",
    "\n",
    "```\n",
    "Frame t=0 (\"prev\"):          Frame t=1 (\"next\"):\n",
    "(0,0)=10 (1,0)=20 (2,0)=30   (0,0)=0  (1,0)=10 (2,0)=20\n",
    "```\n",
    "\n",
    "Optical flow:\n",
    "\n",
    "```\n",
    "u = 1 everywhere\n",
    "v = 0 everywhere\n",
    "```\n",
    "\n",
    "That means:\n",
    "\n",
    "* pixel at (0,0) in prev → (1,0) in next\n",
    "* pixel at (1,0) in prev → (2,0) in next\n",
    "* pixel at (2,0) in prev → (3,0) in next (off-screen)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23663f1-0867-408e-afad-375098212748",
   "metadata": {},
   "source": [
    "#### **Warped Previous Frame**\n",
    "![](images/5.Warped_Previous_frame.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e14663c-b7c4-4286-be7a-e5d3eadc635c",
   "metadata": {},
   "source": [
    "#### **Difference Next Frame Warped Previous Frame**\n",
    "\n",
    "\n",
    "\n",
    "**Ideally, `Difference = Next Frame - Warped Previous Frame` should be zero (a completely black image).**\n",
    "\n",
    "**Why?**\n",
    "- If the optical flow perfectly captures all the motion between frames\n",
    "- And we warp `prev` by those exact flow vectors\n",
    "- Then `warped_prev` should look **identical** to `next`\n",
    "- So their difference would be zero\n",
    "\n",
    "**In reality, the difference is rarely zero because:**\n",
    "\n",
    "1. **Flow estimation errors** - The optical flow algorithm makes approximations\n",
    "2. **Occlusions** - Objects that appear or disappear between frames (one frame sees them, the other doesn't)\n",
    "3. **Lighting changes** - Brightness variations that aren't motion\n",
    "4. **Non-rigid motion** - Deformations that don't follow the simple displacement model\n",
    "5. **Motion boundaries** - Areas where objects move at different speeds create discontinuities\n",
    "6. **Noise** - Camera sensor noise\n",
    "\n",
    "**The difference image is actually very useful for:**\n",
    "- **Quality assessment**: Smaller difference = better optical flow estimation\n",
    "- **Error analysis**: Bright regions show where the flow failed\n",
    "- **Debugging**: Helps identify problematic areas in the motion estimation\n",
    "\n",
    "So when you run the code, pay attention to the difference image - the darker it is overall, the better your optical flow worked!\n",
    "\n",
    "\n",
    "\n",
    "![](images/6.Difference_(Next_Warped_Prev).png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8210615c-1bc4-46b4-9765-acf2b216093c",
   "metadata": {},
   "source": [
    "\n",
    "### TL;DR\n",
    "\n",
    "| Symbol            | Meaning                                                     |\n",
    "| ----------------- | ----------------------------------------------------------- |\n",
    "| `flow[y,x,0] = u` | Horizontal motion (pixels moved right by u)                 |\n",
    "| `flow[y,x,1] = v` | Vertical motion (pixels moved down by v)                    |\n",
    "| Equation          | (x’, y’) = (x + u, y + v)                                   |\n",
    "| So yes            | “Pixel at prev(x,y)” moved to “next(x+u, y+v)”              |\n",
    "| Therefore         | To find its previous location → use inverse map: (x-u, y-v) |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
