{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e942c481-8a85-4cb8-b0a8-5457f644d800",
   "metadata": {},
   "source": [
    "# What is a cost volume?\n",
    "\n",
    "For each target pixel $(u,v)$ and a set of hypothesized depths $\\{d_i\\}_{i=1}^D$, you measure how well that pixel matches across nearby frames when you **warp** those frames as if the pixel were at depth $d_i$. Stack those per-depth match costs into a 3D tensor $C \\in \\mathbb{R}^{D\\times H\\times W}$. A network regularizes $C$ and reads out depth (and sometimes confidence).\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img\n",
    "    src=\"images/Diagram-showing-how-the-cost-volume-is-constructed.-1024x570.jpg\"\n",
    "    alt=\"cost volume\" />\n",
    "  <figcaption>cost volume, image courtesy learnopencv</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "# How you build it (plane-sweep, differentiable)\n",
    "\n",
    "1. **Features**\n",
    "   Extract multi-scale features $F_t, F_s \\in \\mathbb{R}^{C\\times H\\times W}$ from target $I_t$ and source $I_s$ (ResNet/ConvNeXt/Lightweight encoder).\n",
    "\n",
    "2. **Depth (usually inverse-depth) hypotheses**\n",
    "   Choose $D$ planes: $z_i$ (or $1/z$) between near/far (log or inverse-depth spacing is best).\n",
    "\n",
    "3. **Warp source features per plane**\n",
    "   For each depth $z_i$ and pixel $p=(u,v)$:\n",
    "\n",
    "* Back-project: $\\mathbf{X} = z_i K^{-1}\\tilde{p}$\n",
    "* Transform: $\\mathbf{X}_s = T_{t\\rightarrow s}\\mathbf{X}$\n",
    "* Project: $p' \\sim K\\,\\mathbf{X}_s$\n",
    "* Sample: $F_s^{(i)}(p) = \\text{bilinear\\_sample}(F_s, p')$\n",
    "\n",
    "4. **Compute matching cost** (several good options)\n",
    "\n",
    "* **L1/L2 concatenation:** $C_i(u,v) = \\| F_t(u,v) - F_s^{(i)}(u,v)\\|_1$\n",
    "* **Dot-product correlation:** $C_i(u,v) = -\\langle F_t, F_s^{(i)}\\rangle$\n",
    "* **Groupwise correlation:** split channels into $G$ groups, correlate per group, then average (cheap & strong).\n",
    "* **Variance/mean aggregation** across multiple sources $\\{s\\}$: per-plane mean/variance of warped features.\n",
    "\n",
    "Stack over depths → **raw cost volume** $C$.\n",
    "\n",
    "5. **Regularize (denoise, complete, enforce smooth structure)**\n",
    "\n",
    "* **3D CNN encoder–decoder (hourglass)** over $(D,H,W)$ (classic in stereo/MVS).\n",
    "* **2D CNN + guided aggregation/attention** (lighter memory).\n",
    "* **Recurrent/transformer** updates with a **correlation pyramid** (RAFT/DROID-style).\n",
    "\n",
    "6. **Depth readout**\n",
    "\n",
    "* **Softmax along depth**: $P_i(u,v)=\\text{softmax}(-C)_i$\n",
    "* **Soft-argmin**: $\\hat{z}(u,v)=\\sum_i P_i(u,v)\\,z_i$ (use inverse-depth to reduce bias).\n",
    "* Output optional **confidence** (entropy of $P$ or learned head).\n",
    "\n",
    "# Where pose comes from in self-supervised Depth+Pose\n",
    "\n",
    "* **Option 1 (common):** a small PoseNet predicts $T_{t\\rightarrow s}$. You **use that pose** to build the cost volume for depth, then train end-to-end with photometric + SSIM + smoothness losses using the predicted depth.\n",
    "* **Option 2 (tighter geometry):** alternate/refine pose via a differentiable PnP/BA block using correspondences implied by the current depth/posterior $P$. (Heavier but more accurate.)\n",
    "\n",
    "# Training losses that pair well\n",
    "\n",
    "* **Photometric reconstruction:** $\\alpha\\frac{1-\\text{SSIM}(I_t,\\hat{I}_t)}{2} + (1-\\alpha)\\|I_t-\\hat{I}_t\\|_1$, where $\\hat{I}_t$ is source warped with $\\hat{z}$ and pose.\n",
    "* **Edge-aware smoothness** on inverse depth.\n",
    "* **Multi-view min-reprojection** to handle occlusions.\n",
    "* **Pose/geodesic regularization** (optional).\n",
    "* **Depth priors** if you have ToF/RGB-D: narrow-band volume around measured depth; penalize deviation within sensor confidence.\n",
    "\n",
    "# Variants & design choices\n",
    "\n",
    "* **Stereo only:** disparity cost volume is 1D shifts → very efficient (PSMNet/GA-Net style); treat disparity bins instead of depth.\n",
    "* **Temporal monocular (your case for self-sup):** plane-sweep with predicted pose between adjacent frames; inverse-depth bins.\n",
    "* **Multi-view (MVS):** aggregate costs from many neighbors with per-plane mean/variance; strong for wide baselines.\n",
    "* **Correlation pyramids (RAFT-like):** store all-pairs correlations at multiple scales and iterate updates with a GRU—memory-savvy and very accurate.\n",
    "* **Groupwise correlation:** big win on speed/accuracy trade-off; keep it as your default matcher.\n",
    "* **3D vs 2D regularization:** 3D CNNs are powerful but memory-heavy; 2D with smart aggregation or recurrent updates often suffices.\n",
    "\n",
    "# Practical settings that work\n",
    "\n",
    "* **Depth range:** pick near/far from your camera (e.g., 0.3–30 m); **inverse-depth** spacing.\n",
    "* **Pyramids:** at 1/8 or 1/4 scale use $D=32\\!-\\!64$ planes; refine at higher res in a **coarse-to-fine** scheme with a **narrow-band** (±3–6 planes) around the upsampled estimate.\n",
    "* **Occlusions:** per-plane z-buffer (keep nearest), or **min-reprojection** across sources; add auto-mask for moving objects.\n",
    "* **Numerics:** ensure intrinsics are scaled with feature maps; use `grid_sample(align_corners=True/False)` consistently; mask out-of-bounds warps; mixed precision + checkpointing.\n",
    "* **Confidence:** use entropy of $P$ to down-weight photometric residuals; improves robustness.\n",
    "\n",
    "# Minimal pseudo-PyTorch sketch (core ideas)\n",
    "\n",
    "```python\n",
    "# F_t, F_s: [B,C,H,W] features; depths: [D] inverse-depth or depth values\n",
    "# K, T_ts: intrinsics (scaled to H,W), relative pose t->s\n",
    "C = []  # cost volume planes\n",
    "for z in depths:  # vectorize in practice\n",
    "    # backproject target pixels at depth z, transform to source, project\n",
    "    p_src = project(T_ts, backproject(grid(H,W), z, K), K)    # [B,2,H,W]\n",
    "    F_sw = bilinear_sample(F_s, p_src)                        # [B,C,H,W]\n",
    "    # groupwise correlation (G groups):\n",
    "    cost = groupwise_corr(F_t, F_sw, G=8)                     # [B,1,H,W]\n",
    "    C.append(cost)\n",
    "C = torch.stack(C, dim=1)                                     # [B,D,H,W]\n",
    "\n",
    "# 3D regularization\n",
    "R = hourglass3d(C)                                            # [B,D,H,W]\n",
    "P = torch.softmax(-R, dim=1)                                  # prob over depth\n",
    "z_hat = (P * depths.view(1,-1,1,1)).sum(dim=1)                # [B,H,W]\n",
    "```\n",
    "\n",
    "# How this fits your plan (Depth+Pose self-sup)\n",
    "\n",
    "1. Target & 2–3 neighbor frames → PoseNet for $T_{t\\rightarrow s}$.\n",
    "2. Build **inverse-depth** cost volume via plane sweep.\n",
    "3. 3D (or 2D+recurrent) regularization → **probability volume** → depth.\n",
    "4. Losses: photometric (min over sources) + SSIM + smoothness (+ pose reg).\n",
    "5. Optional: small pose refinement block using current depth/confidence.\n",
    "\n",
    "# If you have ToF/RGB-D later\n",
    "\n",
    "Use the sensor depth as:\n",
    "\n",
    "* **Narrow-band anchor:** build the volume only around $z_\\text{ToF} \\pm \\delta$.\n",
    "* **Depth completion:** fuse ToF depth into the 3D regularizer (concat a confidence map).\n",
    "* **Hard constraints:** penalize deviation where ToF is confident; ignore where invalid.\n",
    "\n",
    "Ref: [1](https://www.youtube.com/watch?v=lBFgNyz5JpU)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
