{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9ba81e-ba31-411e-82b1-2b641c7fcd8c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 1. **BFMatcher (Brute-Force Matcher)**\n",
    "   - Compares descriptors between keypoints in a brute-force manner.\n",
    "   - Supports different norms for distance calculation, such as `cv::NORM_L2` or `cv::NORM_HAMMING`.\n",
    "   - Suitable for small datasets.\n",
    "   - Example:\n",
    "     ```cpp\n",
    "     cv::BFMatcher matcher(cv::NORM_L2);\n",
    "     std::vector<cv::DMatch> matches;\n",
    "     matcher.match(descriptors1, descriptors2, matches);\n",
    "     ```\n",
    "\n",
    "\n",
    "#### Distance Types\n",
    "- `L2 Norm` : `cv.NORM_L2` Euclidean distance, used mainly for floating-point descriptors like SIFT and SURF.\n",
    "By default, it is `cv.NORM_L2`. It is good for SIFT, SURF etc (`cv.NORM_L1 is` also there). \n",
    "\n",
    "```\n",
    "bf = cv2.BFMatcher(cv2.cv.NORM_L2, crossCheck=True)\n",
    "```\n",
    "\n",
    "\n",
    "- `Hamming Distance`: `cv.NORM_HAMMING` Used for binary string-based descriptors like `ORB, BRIEF, and BRISK`. It counts the number of differing bits between two binary strings. If ORB is using `WTA_K == 3` or `4`, which takes 3 or 4 points to produce BRIEF descriptor, `cv.NORM_HAMMING2` should be used.\n",
    "\n",
    "\n",
    "```\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "```\n",
    "\n",
    "- `Cross Check`: An option in the BFMatcher that ensures mutual matching. For two keypoints to be considered a match, the keypoint in the first image must match the keypoint in the second image, and vice-versa.\n",
    "\n",
    "\n",
    "### 2. **FlannBasedMatcher (Fast Library for Approximate Nearest Neighbors)**\n",
    "   - Efficient for large datasets by using an approximate nearest neighbor algorithm.\n",
    "   - Suitable for high-dimensional data and faster than BFMatcher for larger datasets.\n",
    "   - Example:\n",
    "     ```cpp\n",
    "     cv::FlannBasedMatcher matcher;\n",
    "     std::vector<cv::DMatch> matches;\n",
    "     matcher.match(descriptors1, descriptors2, matches);\n",
    "     ```\n",
    "\n",
    "**Additional Matching Techniques**\n",
    "   OpenCV also allows for custom matchers or modifications:\n",
    "   \n",
    "   - **Radius Match**: Finds all descriptors within a specified distance.\n",
    "     ```cpp\n",
    "     matcher.radiusMatch(descriptors1, descriptors2, knnMatches, maxDistance);\n",
    "     ```\n",
    "\n",
    "   - **K-Nearest Neighbors (KNN) Match**: Finds the k best matches for each descriptor.\n",
    "     ```cpp\n",
    "     matcher.knnMatch(descriptors1, descriptors2, knnMatches, k);\n",
    "     ```\n",
    "\n",
    "   - **Cross-Check Matching**: A refinement where matches are validated bidirectionally.\n",
    "\n",
    "### Summary of Norm Types for Descriptors\n",
    "   The choice of matcher depends on the type of descriptor used:\n",
    "   - **SIFT/SURF**: Use `cv::NORM_L2` with BFMatcher or FlannBasedMatcher.\n",
    "   - **ORB/BRIEF/BRISK**: Use `cv::NORM_HAMMING` or `cv::NORM_HAMMING2` with BFMatcher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec2511b-6924-46e0-ab3a-f2bebc6bedf7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Example Using  Matchers\n",
    "Here’s a combined example with `BFMatcher` and `FlannBasedMatcher`:\n",
    "```cpp\n",
    "#include <opencv2/opencv.hpp>\n",
    "#include <opencv2/features2d.hpp>\n",
    "\n",
    "int main() {\n",
    "    cv::Mat img1 = cv::imread(\"image1.jpg\", cv::IMREAD_GRAYSCALE);\n",
    "    cv::Mat img2 = cv::imread(\"image2.jpg\", cv::IMREAD_GRAYSCALE);\n",
    "\n",
    "    cv::Ptr<cv::Feature2D> detector = cv::ORB::create();\n",
    "    std::vector<cv::KeyPoint> keypoints1, keypoints2;\n",
    "    cv::Mat descriptors1, descriptors2;\n",
    "\n",
    "    detector->detectAndCompute(img1, cv::noArray(), keypoints1, descriptors1);\n",
    "    detector->detectAndCompute(img2, cv::noArray(), keypoints2, descriptors2);\n",
    "\n",
    "    // BFMatcher\n",
    "    cv::BFMatcher bfMatcher(cv::NORM_HAMMING);\n",
    "    std::vector<cv::DMatch> bfMatches;\n",
    "    bfMatcher.match(descriptors1, descriptors2, bfMatches);\n",
    "\n",
    "    // FlannBasedMatcher\n",
    "    cv::FlannBasedMatcher flannMatcher;\n",
    "    std::vector<cv::DMatch> flannMatches;\n",
    "    flannMatcher.match(descriptors1, descriptors2, flannMatches);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe39e5d2-2214-4637-82bb-87c0d73eb720",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Full Example with `knnMatch`\n",
    "\n",
    "```cpp\n",
    "#include <opencv2/opencv.hpp>\n",
    "#include <opencv2/features2d.hpp>\n",
    "#include <iostream>\n",
    "\n",
    "int main() {\n",
    "    // Read input images\n",
    "    cv::Mat img1 = cv::imread(\"image1.jpg\", cv::IMREAD_GRAYSCALE);\n",
    "    cv::Mat img2 = cv::imread(\"image2.jpg\", cv::IMREAD_GRAYSCALE);\n",
    "\n",
    "    if (img1.empty() || img2.empty()) {\n",
    "        std::cerr << \"Could not load images!\" << std::endl;\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    // Detect ORB keypoints and compute descriptors\n",
    "    cv::Ptr<cv::ORB> detector = cv::ORB::create();\n",
    "    std::vector<cv::KeyPoint> keypoints1, keypoints2;\n",
    "    cv::Mat descriptors1, descriptors2;\n",
    "\n",
    "    detector->detectAndCompute(img1, cv::noArray(), keypoints1, descriptors1);\n",
    "    detector->detectAndCompute(img2, cv::noArray(), keypoints2, descriptors2);\n",
    "\n",
    "    if (descriptors1.empty() || descriptors2.empty()) {\n",
    "        std::cerr << \"Descriptors could not be computed!\" << std::endl;\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    // Use BFMatcher with NORM_HAMMING (suitable for ORB descriptors)\n",
    "    cv::BFMatcher matcher(cv::NORM_HAMMING, /*crossCheck=*/false);\n",
    "\n",
    "    // Perform KNN matching (k=2)\n",
    "    std::vector<std::vector<cv::DMatch>> knnMatches;\n",
    "    matcher.knnMatch(descriptors1, descriptors2, knnMatches, 2);\n",
    "\n",
    "    // Apply Lowe's ratio test to filter matches\n",
    "    const float ratioThresh = 0.75f; // Lowe's ratio test threshold\n",
    "    std::vector<cv::DMatch> goodMatches;\n",
    "    for (const auto& knnMatch : knnMatches) {\n",
    "        if (knnMatch.size() >= 2 && knnMatch[0].distance < ratioThresh * knnMatch[1].distance) {\n",
    "            goodMatches.push_back(knnMatch[0]);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Draw matches\n",
    "    cv::Mat imgMatches;\n",
    "    cv::drawMatches(img1, keypoints1, img2, keypoints2, goodMatches, imgMatches,\n",
    "                    cv::Scalar::all(-1), cv::Scalar::all(-1), std::vector<char>(),\n",
    "                    cv::DrawMatchesFlags::NOT_DRAW_SINGLE_POINTS);\n",
    "\n",
    "    // Display the result\n",
    "    cv::imshow(\"Good Matches\", imgMatches);\n",
    "    cv::waitKey(0);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### Explanation of Key Steps:\n",
    "1. **Feature Detection and Description**:\n",
    "   - `cv::ORB::create()` detects ORB keypoints and computes descriptors.\n",
    "\n",
    "2. **Matcher Selection**:\n",
    "   - `cv::BFMatcher` with `cv::NORM_HAMMING` is used, as ORB descriptors are binary.\n",
    "\n",
    "3. **KNN Matching**:\n",
    "   - `matcher.knnMatch()` finds the two best matches for each descriptor.\n",
    "\n",
    "4. **Lowe's Ratio Test**:\n",
    "   - Compares the distances of the two best matches to filter out ambiguous matches.\n",
    "\n",
    "5. **Visualization**:\n",
    "   - `cv::drawMatches()` visualizes the filtered matches.\n",
    "\n",
    "### Customization:\n",
    "- Replace `cv::ORB` with another feature detector like `cv::SIFT` or `cv::AKAZE` as needed.\n",
    "- Adjust `ratioThresh` for stricter or more lenient filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8596337e-1975-41e7-9c31-a3edfd246c5a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Example of `radiusMatch` (Complete Code):\n",
    "\n",
    "```cpp\n",
    "#include <opencv2/opencv.hpp>\n",
    "#include <opencv2/features2d.hpp>\n",
    "#include <iostream>\n",
    "\n",
    "int main() {\n",
    "    // Read input images\n",
    "    cv::Mat img1 = cv::imread(\"image1.jpg\", cv::IMREAD_GRAYSCALE);\n",
    "    cv::Mat img2 = cv::imread(\"image2.jpg\", cv::IMREAD_GRAYSCALE);\n",
    "\n",
    "    if (img1.empty() || img2.empty()) {\n",
    "        std::cerr << \"Could not load images!\" << std::endl;\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    // Detect ORB keypoints and compute descriptors\n",
    "    cv::Ptr<cv::ORB> detector = cv::ORB::create();\n",
    "    std::vector<cv::KeyPoint> keypoints1, keypoints2;\n",
    "    cv::Mat descriptors1, descriptors2;\n",
    "\n",
    "    detector->detectAndCompute(img1, cv::noArray(), keypoints1, descriptors1);\n",
    "    detector->detectAndCompute(img2, cv::noArray(), keypoints2, descriptors2);\n",
    "\n",
    "    if (descriptors1.empty() || descriptors2.empty()) {\n",
    "        std::cerr << \"Descriptors could not be computed!\" << std::endl;\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    // Use BFMatcher with NORM_HAMMING (suitable for ORB descriptors)\n",
    "    cv::BFMatcher matcher(cv::NORM_HAMMING);\n",
    "\n",
    "    // Perform radius matching\n",
    "    const float maxDistance = 50.0f; // Radius threshold\n",
    "    std::vector<std::vector<cv::DMatch>> radiusMatches;\n",
    "    matcher.radiusMatch(descriptors1, descriptors2, radiusMatches, maxDistance);\n",
    "\n",
    "    // Filter and collect matches for visualization\n",
    "    std::vector<cv::DMatch> goodMatches;\n",
    "    for (const auto& matches : radiusMatches) {\n",
    "        for (const auto& match : matches) {\n",
    "            if (match.distance < maxDistance) {\n",
    "                goodMatches.push_back(match);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Draw matches\n",
    "    cv::Mat imgMatches;\n",
    "    cv::drawMatches(img1, keypoints1, img2, keypoints2, goodMatches, imgMatches);\n",
    "\n",
    "    // Display the result\n",
    "    cv::imshow(\"Good Matches (Radius Match)\", imgMatches);\n",
    "    cv::waitKey(0);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### When to Use Which:\n",
    "- Use `knnMatch` when you need a fixed number of best matches for each descriptor (e.g., for applying Lowe's ratio test).\n",
    "- Use `radiusMatch` when you want to consider all matches within a spatial or distance threshold, especially for applications where proximity is more critical than ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e98bb-f9a9-4475-b754-44fac336d002",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The **distance** in OpenCV's matchers (such as `knnMatch` or `radiusMatch`) is not measured in pixels. Instead, it represents the **difference between feature descriptors**. The nature of this distance depends on:\n",
    "\n",
    "### 1. **Descriptor Type**\n",
    "   - The type of feature descriptor (e.g., ORB, SIFT, SURF) determines how the distance is calculated.\n",
    "\n",
    "### 2. **Norm Type (Distance Metric)**\n",
    "   - When creating a matcher (e.g., `cv::BFMatcher`), you specify a **norm type**, which defines the mathematical metric used to compute the distance. Common norms include:\n",
    "     - **`cv::NORM_L2` (Euclidean Distance):**\n",
    "       - Used for floating-point descriptors like SIFT and SURF.\n",
    "       - Distance = √(Σ(descriptor1[i] - descriptor2[i])²)\n",
    "     - **`cv::NORM_HAMMING` (Hamming Distance):**\n",
    "       - Used for binary descriptors like ORB, BRIEF, and BRISK.\n",
    "       - Distance = Number of differing bits between two binary descriptors.\n",
    "\n",
    "### 3. **Physical Meaning**\n",
    "   - The \"distance\" is purely a measure of similarity between feature descriptors:\n",
    "     - Smaller distance = Higher similarity.\n",
    "     - Larger distance = Lower similarity.\n",
    "   - It does not directly correspond to spatial distances in pixels.\n",
    "\n",
    "### Example Interpretation:\n",
    "- **ORB Descriptor with `cv::NORM_HAMMING`:**\n",
    "  - A distance of `5` means there are 5 differing bits between the binary descriptors.\n",
    "- **SIFT Descriptor with `cv::NORM_L2`:**\n",
    "  - A distance of `10.5` means the Euclidean distance between two descriptors in the feature space is `10.5`.\n",
    "\n",
    "### Choosing Distance Thresholds:\n",
    "- When filtering matches (e.g., in `radiusMatch` or Lowe's ratio test), the distance threshold depends on the descriptor type:\n",
    "  - **ORB (Binary Descriptors):** A typical `maxDistance` is in the range of `30–50` (Hamming distance).\n",
    "  - **SIFT/SURF (Floating-Point Descriptors):** A typical `maxDistance` might be `0.5–1.0` (L2 norm).\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "If you’re using ORB and set `maxDistance = 50` in `radiusMatch`:\n",
    "- The matcher will consider all descriptors from the second image that are within 50 bits difference (Hamming distance) of a descriptor from the first image.\n",
    "\n",
    "If you’re using SIFT and set `maxDistance = 1.0`:\n",
    "- The matcher will consider all descriptors within a Euclidean distance of 1.0.\n",
    "\n",
    "### Summary:\n",
    "The \"distance\" reflects the difference between descriptors in their respective feature spaces, **not physical pixel distances in the image**. It is a similarity measure for matching purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352c83a-ae16-47a6-9a8e-d3f2cefcc47e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Lowe's ratio test is a widely-used method to filter out ambiguous or incorrect matches when performing feature matching. Here's a detailed explanation of what it is, how it works, and why we compare only `knnMatch[0].distance` with `ratioThresh * knnMatch[1].distance`.\n",
    "\n",
    "---\n",
    "\n",
    "### What is Lowe's Ratio Test?\n",
    "\n",
    "- It is a **validation step** that helps distinguish between good matches (true correspondences) and poor or ambiguous matches (false correspondences).\n",
    "- The idea is based on the assumption that a good match should have a **significantly better similarity score** (smaller distance) than the next best alternative match.\n",
    "\n",
    "---\n",
    "\n",
    "### How Does it Work?\n",
    "\n",
    "1. **`knnMatch` Results**:\n",
    "   - For each descriptor in the first image, the `knnMatch` function retrieves the top `k` closest matches from the descriptors in the second image.\n",
    "   - Typically, `k=2` is used to retrieve the two best matches: `knnMatch[0]` (best match) and `knnMatch[1]` (second-best match).\n",
    "\n",
    "2. **Ratio Test**:\n",
    "   - Compare the distance of the best match (`knnMatch[0].distance`) to the distance of the second-best match (`knnMatch[1].distance`).\n",
    "   - If the ratio of these distances is below a predefined threshold (e.g., 0.75), it indicates a strong match. Otherwise, the match is considered ambiguous and discarded.\n",
    "\n",
    "   Mathematically:\n",
    "   $\n",
    "   \\text{If } \\frac{\\text{knnMatch[0].distance}}{\\text{knnMatch[1].distance}} < \\text{ratioThresh}, \\text{ accept the match.}\n",
    "   $\n",
    "\n",
    "   - **Typical `ratioThresh` Values**:\n",
    "     - The default threshold is often **0.7** or **0.75** (Lowe's original paper recommends 0.75).\n",
    "\n",
    "---\n",
    "\n",
    "### Why Compare Only the Two Best Matches?\n",
    "\n",
    "The comparison between the best match (`knnMatch[0]`) and the second-best match (`knnMatch[1]`) provides a measure of confidence in the match:\n",
    "\n",
    "1. **Ambiguity Detection**:\n",
    "   - If the best match (`knnMatch[0]`) is very similar in distance to the second-best match (`knnMatch[1]`), the descriptor is likely ambiguous.\n",
    "   - Ambiguity often occurs when two or more features in the second image are very similar to a feature in the first image.\n",
    "\n",
    "2. **Robustness to Noise**:\n",
    "   - By considering only the top two matches, the test avoids errors introduced by noisy or irrelevant descriptors.\n",
    "\n",
    "3. **Efficiency**:\n",
    "   - Examining just the top two matches reduces computational complexity while still providing robust filtering.\n",
    "\n",
    "---\n",
    "\n",
    "### Example of Lowe's Ratio Test in Practice:\n",
    "\n",
    "```cpp\n",
    "const float ratioThresh = 0.75f; // Lowe's ratio threshold\n",
    "std::vector<cv::DMatch> goodMatches;\n",
    "\n",
    "for (const auto& knnMatch : knnMatches) {\n",
    "    if (knnMatch.size() >= 2) { // Ensure at least two matches exist\n",
    "        // Apply Lowe's ratio test\n",
    "        if (knnMatch[0].distance < ratioThresh * knnMatch[1].distance) {\n",
    "            goodMatches.push_back(knnMatch[0]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Visualization:\n",
    "\n",
    "#### Example:\n",
    "Imagine the distances for a descriptor in image 1 are:\n",
    "- `knnMatch[0].distance = 0.3` (best match)\n",
    "- `knnMatch[1].distance = 0.5` (second-best match)\n",
    "\n",
    "$\n",
    "\\text{Ratio: } \\frac{0.3}{0.5} = 0.6\n",
    "$\n",
    "\n",
    "If `ratioThresh = 0.75`, this match passes the test.\n",
    "\n",
    "#### Counterexample:\n",
    "Now imagine the distances are:\n",
    "- `knnMatch[0].distance = 0.4`\n",
    "- `knnMatch[1].distance = 0.42`\n",
    "\n",
    "$\n",
    "\\text{Ratio: } \\frac{0.4}{0.42} \\approx 0.95\n",
    "$\n",
    "\n",
    "This match fails the test because the best match is not significantly better than the second-best match, indicating ambiguity.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Not Compare More Matches?\n",
    "\n",
    "- Comparing more than two matches would increase computational overhead without significantly improving robustness.\n",
    "- Lowe's ratio test is designed to identify **clear, unambiguous matches**. If the first two matches are ambiguous, adding more matches is unlikely to resolve the ambiguity.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "- Lowe's ratio test ensures that the best match is **distinctively better** than the second-best match, reducing false positives.\n",
    "- It relies only on `knnMatch[0]` and `knnMatch[1]` for simplicity and efficiency.\n",
    "- The ratio threshold (e.g., `0.75`) is an empirically determined value that balances filtering out ambiguous matches and retaining valid ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e83493a-ddeb-446f-ad91-0a59b36c78a2",
   "metadata": {},
   "source": [
    "[c++ code](../src/correspondences_matching.cpp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
