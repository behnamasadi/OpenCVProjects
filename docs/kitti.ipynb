{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60b1fbd-ac67-4f1e-a91f-6bcf6bfd6671",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "1. `ground truth poses` can be downloaded from [here](http://www.cvlibs.net/download.php?file=data_odometry_poses.zip)\n",
    "2. `left images`, `right images` and `time stamp` can be downloaded from [here](http://www.cvlibs.net/download.php?file=data_odometry_gray.zip)\n",
    "3. `cameras calibration file` can be downloaded from [here](http://www.cvlibs.net/download.php?file=data_odometry_calib.zip)\n",
    "\n",
    "\n",
    "\n",
    "# Sensor setup \n",
    "<img src=\"images/setup_top_view.png\" />\n",
    "\n",
    "\n",
    "- $P0$: Reference camera (left of stereo pair 1), extrinsics are identity.\n",
    "- $P1$: Right camera of stereo pair 1, extrinsics include baseline offset.\n",
    "- $P2$: Left camera of stereo pair 2, extrinsics depend on setup.\n",
    "- $P3$: Right camera of stereo pair 2, extrinsics depend on setup.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Camera: $P0$:\n",
    "\n",
    "```\n",
    "Projection Matrix:\n",
    "[[707.0912   0.     601.8873   0.    ]\n",
    " [  0.     707.0912 183.1104   0.    ]\n",
    " [  0.       0.       1.       0.    ]]\n",
    "Intrinsic Matrix:\n",
    "[[707.0912   0.     601.8873]\n",
    " [  0.     707.0912 183.1104]\n",
    " [  0.       0.       1.    ]]\n",
    "Rotation Matrix:\n",
    "[[1. 0. 0.]\n",
    " [0. 1. 0.]\n",
    " [0. 0. 1.]]\n",
    "Translation Vector:\n",
    "[[0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "```\n",
    "---\n",
    "\n",
    "Camera: $P1$:\n",
    "```\n",
    "Projection Matrix:\n",
    "[[ 707.0912    0.      601.8873 -379.8145]\n",
    " [   0.      707.0912  183.1104    0.    ]\n",
    " [   0.        0.        1.        0.    ]]\n",
    "Intrinsic Matrix:\n",
    "[[707.0912   0.     601.8873]\n",
    " [  0.     707.0912 183.1104]\n",
    " [  0.       0.       1.    ]]\n",
    "Rotation Matrix:\n",
    "[[1. 0. 0.]\n",
    " [0. 1. 0.]\n",
    " [0. 0. 1.]]\n",
    "Translation Vector:\n",
    "[[ 5.37150653e-01]\n",
    " [-1.34802944e-17]\n",
    " [ 0.00000000e+00]]\n",
    "```\n",
    "\n",
    "From the above image the distance between two camera is `0.54` on $x$ axis and from decomposition we have: `5.37150653e-01`.\n",
    "\n",
    "Refs: [1](https://www.cvlibs.net/datasets/kitti/setup.php)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71705b1a-2cd5-4af1-afe5-02d8df2ae9e0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Ground Truth Poses\n",
    "each row of the data has 12 columns, 12 come from flattening a `3x4` transformation matrix of the left:\n",
    "\n",
    "```\n",
    "r11 r12 r13 tx r21 r22 r23 ty r31 r32 r33 tz\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# stereo camera with respect to the global coordinate frame.\n",
    "\n",
    "Refs: [1](https://stackoverflow.com/questions/60639665/visual-odometry-kitti-dataset)\n",
    "\n",
    "poses = pd.read_csv(\n",
    "    ground_truth_poses, delimiter=' ', header=None)\n",
    "print('Shape of position dataframe:', poses.shape)\n",
    "\n",
    "print('First position:')\n",
    "first_pose = np.array(poses.iloc[0]).reshape((3, 4)).round(2)\n",
    "print(first_pose)\n",
    "\n",
    "\n",
    "gt = np.zeros((len(poses), 3, 4))\n",
    "for i in range(len(poses)):\n",
    "    gt[i] = np.array(poses.iloc[i]).reshape((3, 4))\n",
    "\n",
    "gt[1].dot(np.array([0, 0, 0, 1]))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(7, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(gt[:, :, 3][:, 0], gt[:, :, 3][:, 1], gt[:, :, 3][:, 2])\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.view_init(elev=-40, azim=270)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "left_image_files = [os.path.abspath(os.path.join(\n",
    "    left_images_path, p)) for p in os.listdir(left_images_path)]\n",
    "# sorted(os.listdir(left_images_path))\n",
    "left_image_files.sort()\n",
    "\n",
    "\n",
    "# left_image_files = os.listdir(left_images_path)\n",
    "\n",
    "\n",
    "######################### Time Stamp #########################\n",
    "\n",
    "\n",
    "times = pd.read_csv(time_stamp_path,\n",
    "                    delimiter=' ', header=None)\n",
    "\n",
    "\n",
    "######################### Projection Matrices/ LIDAR #########################\n",
    "# Matrices for 4 cameras projection,  3x4 projection matrices, P0, P1, P2, P3, Tr(LIDAR)\n",
    "\n",
    "calib = pd.read_csv(cameras_file_path,\n",
    "                    delimiter=' ', header=None, index_col=0)\n",
    "\n",
    "P0 = np.array(calib.loc['P0:']).reshape((3, 4))\n",
    "print(P0)\n",
    "\n",
    "P1 = np.array(calib.loc['P1:']).reshape((3, 4))\n",
    "print(P1)\n",
    "\n",
    "\n",
    "# decomposition of a projection matrix into a calibration and a rotation matrix and the position of a camera.\n",
    "# It optionally returns three rotation matrices, one for each axis,\n",
    "cameraMatrix, rotMatrix, transVect, rotMatrixX, rotMatrixY, rotMatrixZ, eulerAngles = cv2.decomposeProjectionMatrix(\n",
    "    P1)\n",
    "\n",
    "\n",
    "print(rotMatrix)\n",
    "transVect = transVect/transVect[3]\n",
    "print(transVect)\n",
    "\n",
    "\n",
    "# Rectification matrix (stereo cameras only) A rotation matrix aligning the camera coordinate system to the ideal stereo image plane so that epipolar lines in both stereo images are parallel.\n",
    "Rt = np.hstack([rotMatrix, transVect[:3]])\n",
    "print(Rt)\n",
    "\n",
    "\n",
    "# detector_name = 'orb'\n",
    "\n",
    "# if detector_name == 'sift':\n",
    "#     detector = cv2.SIFT_create()\n",
    "# elif detector_name == 'orb':\n",
    "#     detector = cv2.ORB_create()\n",
    "# elif detector_name == 'surf':\n",
    "#     detector = cv2.xfeatures2d.SURF_create()\n",
    "\n",
    "\n",
    "detector = cv2.FastFeatureDetector_create(\n",
    "    threshold=25, nonmaxSuppression=True)\n",
    "\n",
    "feature_params = dict(maxCorners=100,\n",
    "                      qualityLevel=0.3,\n",
    "                      minDistance=7,\n",
    "                      blockSize=7)\n",
    "\n",
    "\n",
    "lk_params = dict(winSize=(21, 21), criteria=(\n",
    "    cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01))\n",
    "\n",
    "# lk_params = dict(winSize=(21, 21), criteria=(\n",
    "#                      cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01))\n",
    "\n",
    "previous_frame = None\n",
    "\n",
    "\n",
    "cameraMatrix, rotMatrix, transVect, rotMatrixX, rotMatrixY, rotMatrixZ, eulerAngles = cv2.decomposeProjectionMatrix(\n",
    "    P0)\n",
    "\n",
    "# np.set_printoptions(suppress=True)\n",
    "\n",
    "np.set_printoptions(suppress=True,  formatter={'float_kind': '{:f}'.format})\n",
    "\n",
    "T_cam_new_in_cam_previous = np.eye(4)\n",
    "T_cam_previous_in_world = np.eye(4)\n",
    "\n",
    "\n",
    "traj = np.zeros(shape=(600, 800, 3))\n",
    "\n",
    "\n",
    "for i, img_name in enumerate(left_image_files):\n",
    "    # print(img_name)\n",
    "    # if (i % 20 == 0):\n",
    "    #     print(i)\n",
    "    current_frame = cv2.imread(img_name, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    keypoints_current_frame = cv2.KeyPoint_convert(\n",
    "        detector.detect(current_frame))\n",
    "\n",
    "    # keypoints_current_frame = detector.detect(current_frame)\n",
    "\n",
    "    # keypoints_current_frame = np.array(\n",
    "    #     [x.pt for x in keypoints_current_frame], dtype=np.float32).reshape(-1, 1, 2)\n",
    "\n",
    "    # keypoints_current_frame = cv2.goodFeaturesToTrack(\n",
    "    #     current_frame, mask=None,  **feature_params)\n",
    "\n",
    "    if previous_frame is None:\n",
    "        keypoints_previous_frame = keypoints_current_frame\n",
    "        previous_frame = current_frame\n",
    "        continue\n",
    "\n",
    "    # cv2.imshow('previous_frame', previous_frame)\n",
    "    # cv2.waitKey(5000)\n",
    "    # cv2.imshow('current_frame', current_frame)\n",
    "    # cv2.waitKey(5000)\n",
    "\n",
    "    # print(\"keypoints_previous_frame:\", keypoints_previous_frame)\n",
    "    # print(\"keypoints_current_frame:\", keypoints_current_frame)\n",
    "\n",
    "    # opticalFlowNextPts, status, err = cv2.calcOpticalFlowPyrLK(\n",
    "    #     previous_frame, current_frame, keypoints_previous_frame, keypoints_current_frame, **lk_params)\n",
    "\n",
    "    opticalFlowNextPts, status, err = cv2.calcOpticalFlowPyrLK(\n",
    "        previous_frame, current_frame, keypoints_previous_frame, None, **lk_params)\n",
    "\n",
    "    # print(\"status.shape: \", status.shape)\n",
    "    # print(\"keypoints_previous_frame.shape: \", keypoints_previous_frame.shape)\n",
    "    # print(\"opticalFlowNextPts.shape: \", opticalFlowNextPts.shape)\n",
    "\n",
    "    keypoints_previous_frame = keypoints_previous_frame.reshape(-1, 1, 2)\n",
    "    opticalFlowNextPts = opticalFlowNextPts.reshape(-1, 1, 2)\n",
    "\n",
    "    good_previous = keypoints_previous_frame[status == 1]\n",
    "    good_new = opticalFlowNextPts[status == 1]\n",
    "\n",
    "    previous_frame = current_frame\n",
    "    keypoints_previous_frame = keypoints_current_frame\n",
    "\n",
    "    # https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#ga13f7e34de8fa516a686a56af1196247f\n",
    "    E, mask = cv2.findEssentialMat(\n",
    "        good_new, good_previous, cameraMatrix, cv2.RANSAC, 0.999, 1.0, None)\n",
    "    # print(\"Essential Matrix:\", E)\n",
    "\n",
    "    # https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#gadb7d2dfcc184c1d2f496d8639f4371c0\n",
    "    retval, R, t, mask = cv2.recoverPose(\n",
    "        E, good_previous, good_new, cameraMatrix)\n",
    "\n",
    "    # print(\"t:\", t)\n",
    "    # # print(\"R:\", R @ np.transpose(R))\n",
    "    # print(\"R:\", R)\n",
    "\n",
    "    T_cam_new_in_cam_previous = np.zeros([4, 4])\n",
    "    T_cam_new_in_cam_previous[:3, :3] = R\n",
    "    T_cam_new_in_cam_previous[:3, 3] = t.ravel()\n",
    "    T_cam_new_in_cam_previous[3, 3] = 1\n",
    "    # print(T_cam_new_in_cam_previous[:3, :3])\n",
    "    # print(\"T_cam_new_in_cam_previous:\", T_cam_new_in_cam_previous)\n",
    "    # print(\"T_cam_previous_in_world:\", T_cam_previous_in_world)\n",
    "\n",
    "    T_cam_new_in_world = T_cam_previous_in_world@T_cam_new_in_cam_previous\n",
    "    x, y, z = T_cam_new_in_world[:3, 3]\n",
    "    print(\"x,y,z:\\n\", x, y, z)\n",
    "    T_cam_previous_in_world = T_cam_new_in_world\n",
    "\n",
    "    cv2.imshow('current_frame', current_frame)\n",
    "    k = cv2.waitKey(1)\n",
    "\n",
    "    center = (round(x + 400), round(z + 500))\n",
    "    print(\"center: \", center)\n",
    "    center_coordinates = (120, 50)\n",
    "\n",
    "    color = (255, 0, 0)\n",
    "    radius = 1\n",
    "    thickness = 2\n",
    "\n",
    "    traj = cv2.circle(traj, center, radius, color, thickness)\n",
    "\n",
    "    cv2.imshow('trajectory', traj)\n",
    "    k = cv2.waitKey(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
