{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ee82b28-28d8-4675-9fcb-34ac27178f03",
   "metadata": {},
   "source": [
    "# Stereo Calibration\n",
    "\n",
    "The function `cv::stereoCalibrate` in OpenCV compute the extrinsic parameters (rotation matrix $ R $ and translation vector $ T $) between the two cameras, as well as the intrinsic. \n",
    "\n",
    "- **Rotation matrix $ R = R_{c_1}^{c_2}$**: Relative orientation  of **Camera 1** expressed in **Camera 2**'s coordinate frame.\n",
    "- **Translation vector $ T = T_{c_1}^{c_2}$**: Relative position (translation)  of **Camera 1** expressed in **Camera 2**'s coordinate frame.\n",
    "\n",
    "\n",
    "So, if you have a 3D point $ P_1 $ in the coordinate frame of Camera 1, you can transform it to the coordinate frame of Camera 2 using the following equation:\n",
    "\n",
    "$\n",
    "P_2 = R \\cdot P_1 + T\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ P_1 $ is the 3D point in Camera 1's frame.\n",
    "- $ P_2 $ is the corresponding 3D point in Camera 2's frame.\n",
    "- $ R $ and $ T $ are the rotation and translation between the two camera frames.\n",
    "\n",
    "```cpp\n",
    "double rms = cv::stereoCalibrate(\n",
    "    std::vector<std::vector<cv::Point3f>>& objectPoints,  // 3D points in the world coordinate\n",
    "    std::vector<std::vector<cv::Point2f>>& imagePoints1,  // 2D points in the first camera image\n",
    "    std::vector<std::vector<cv::Point2f>>& imagePoints2,  // 2D points in the second camera image\n",
    "    cv::Mat& cameraMatrix1,  // Intrinsic matrix for the first camera\n",
    "    cv::Mat& distCoeffs1,    // Distortion coefficients for the first camera\n",
    "    cv::Mat& cameraMatrix2,  // Intrinsic matrix for the second camera\n",
    "    cv::Mat& distCoeffs2,    // Distortion coefficients for the second camera\n",
    "    cv::Size& imageSize,     // Size of the images\n",
    "    cv::Mat& R,              // Rotation matrix between cameras\n",
    "    cv::Mat& T,              // Translation vector between cameras\n",
    "    cv::Mat& E,              // Essential matrix (optional)\n",
    "    cv::Mat& F               // Fundamental matrix (optional)\n",
    "    OutputArray \tperViewErrors,\n",
    "    int \tflags = CALIB_FIX_INTRINSIC,\n",
    "    TermCriteria \tcriteria = TermCriteria(TermCriteria::COUNT+TermCriteria::EPS, 30, 1e-6) \n",
    ");\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Refs: [1](https://www.cs.cmu.edu/~16385/s17/Slides/13.1_Stereo_Rectification.pdf), [2](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#ga3207604e4b1a1758aa66acb6ed5aa65d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112fa574-68e9-4f43-9f58-9cba9b851e9e",
   "metadata": {},
   "source": [
    "<img src=\"images/virtual_stereo.png\" />\n",
    "<img src=\"images/virtual_stereo_ray.png\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d263e4f-3f77-4a8e-983f-ca95965ce351",
   "metadata": {},
   "source": [
    "# Image Rectification\n",
    "Image rectification is transforming an image of a scene into a view that is aligned with a desired coordinate system. The goal of rectification is to remove the effects of camera perspective, rotation, and lens distortion, so that the resulting image has a uniform scale and appears to be captured from a front-facing perspective. \n",
    "\n",
    "\n",
    "In the following:\n",
    " \n",
    "- The camera rotating around the `z` axis.\n",
    "- The virtual image plane at `5°` degree is red and at `90°` is green. \n",
    "- The rectified images are in the blue virtual image plane. \n",
    "- The virtual plane must be parallel to the stereo baseline (orange). \n",
    "\n",
    "\n",
    "|   |   |\n",
    "|---|---|\n",
    "|<img src=\"images/image_rectification_1.png\" alt=\"\" />    |<img src=\"images/image_rectification_8.png\" alt=\"\" />  |\n",
    "|<img src=\"images/image_rectification_20.png\" alt=\"\" />   | <img src=\"images/image_rectification_30.png\" alt=\"\" />  |\n",
    "\n",
    "Refs:  [1](https://www.cs.cmu.edu/~16385/s17/Slides/13.1_Stereo_Rectification.pdf), [2](https://people.scs.carleton.ca/~c_shu/Courses/comp4900d/notes/rectification.pdf)\n",
    "[3](https://www.andreasjakl.com/understand-and-apply-stereo-rectification-for-depth-maps-part-2/)\n",
    "\n",
    "[code](../scripts/image_rectification.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c4186-eff4-4c50-b8c0-0acec0f7a529",
   "metadata": {},
   "source": [
    "# Stereo Rectification\n",
    "\n",
    "If cameras are calibrated:\n",
    "```cpp\n",
    "cv::stereoRectify\t(\tInputArray \tcameraMatrix1,\n",
    "InputArray \tdistCoeffs1,\n",
    "InputArray \tcameraMatrix2,\n",
    "InputArray \tdistCoeffs2,\n",
    "Size \timageSize,\n",
    "InputArray \tR,\n",
    "InputArray \tT,\n",
    "OutputArray \tR1,\n",
    "OutputArray \tR2,\n",
    "OutputArray \tP1,\n",
    "OutputArray \tP2,\n",
    "OutputArray \tQ,\n",
    "int \tflags = CALIB_ZERO_DISPARITY,\n",
    "double \talpha = -1,\n",
    "Size \tnewImageSize = Size(),\n",
    "Rect * \tvalidPixROI1 = 0,\n",
    "Rect * \tvalidPixROI2 = 0 \n",
    ")\t\t\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **`R1` (3×3 Rectification Rotation Matrix for the First Camera):**\n",
    "\n",
    "- **Purpose**: `R1` is a rotation matrix that transforms points from the unrectified coordinate system of the first camera to the rectified coordinate system of the first camera.\n",
    "- **Technical Explanation**:\n",
    "  - A *rectified coordinate system* aligns the epipolar lines (lines along which corresponding points between stereo images lie) to be parallel to the image rows.\n",
    "  - This transformation simplifies stereo correspondence and disparity estimation.\n",
    "  - In essence, `R1` changes the *basis* of points in the 3D space for the first camera so that its coordinate system aligns with the rectified one.\n",
    "- **Use Case**: After applying `R1` to 3D points, they are in the rectified coordinate system for the first camera.\n",
    "\n",
    "---\n",
    "\n",
    "#### **`R2` (3×3 Rectification Rotation Matrix for the Second Camera):**\n",
    "\n",
    "- **Purpose**: Similar to `R1`, `R2` performs a transformation for the second camera.\n",
    "- **Technical Explanation**:\n",
    "  - `R2` aligns the second camera's coordinate system to a rectified frame, ensuring its epipolar lines are also parallel and aligned with those of the first camera.\n",
    "  - This matrix ensures that corresponding points in the second image align row-wise with those in the first image.\n",
    "- **Use Case**: After applying `R2`, the 3D points in the second camera's coordinate system are rectified.\n",
    "\n",
    "---\n",
    "\n",
    "#### **`P1` (3×4 Rectified Projection Matrix for the First Camera):**\n",
    "\n",
    "- **Purpose**: `P1` projects 3D points in the rectified coordinate system of the first camera into its 2D image plane.\n",
    "- **Technical Explanation**:\n",
    "  - A projection matrix maps 3D points $(X, Y, Z)$ to 2D image points $(u, v)$ using the intrinsic camera parameters and extrinsic parameters (rotation and translation).\n",
    "  - `P1` includes the rectified camera matrix (intrinsics of the rectified system) and any additional translation or baseline offset due to rectification.\n",
    "  - `P1` takes points from the rectified coordinate system and produces the corresponding 2D pixel coordinates in the first rectified image.\n",
    "- **Use Case**: Used for projecting rectified 3D points into the rectified image from the first camera.\n",
    "\n",
    "---\n",
    "\n",
    "#### **`P2` (3×4 Rectified Projection Matrix for the Second Camera):**\n",
    "\n",
    "- **Purpose**: Similar to `P1`, `P2` projects 3D points into the 2D image plane of the second camera in the rectified coordinate system.\n",
    "- **Technical Explanation**:\n",
    "  - `P2` contains the intrinsic parameters of the rectified second camera and includes the stereo baseline between the two cameras.\n",
    "  - This matrix accounts for the rectification process and ensures that corresponding points between images are horizontally aligned.\n",
    "- **Use Case**: Used for projecting rectified 3D points into the rectified image from the second camera.\n",
    "\n",
    "---\n",
    "\n",
    "#### **`Q` (4×4 Disparity-to-Depth Mapping Matrix):**\n",
    "\n",
    "- **Purpose**: `Q` maps disparity (difference in horizontal pixel coordinates of a point in the two images) to depth information.\n",
    "- **Technical Explanation**:\n",
    "  - Disparity values $d = u_1 - u_2$ (where $u_1$ and $u_2$ are the horizontal coordinates of a point in the first and second rectified images) are used to compute depth $Z$:\n",
    "    $\n",
    "    Z = \\frac{f \\cdot B}{d}\n",
    "    $\n",
    "    where:\n",
    "    - $f$: Focal length of the rectified cameras.\n",
    "    - $B$: Baseline (distance between the two cameras).\n",
    "  - `Q` encodes this relationship in a single transformation matrix, allowing for efficient computation of 3D points using the `reprojectImageTo3D` function.\n",
    "- **Use Case**: Converts disparity maps into 3D point clouds by mapping disparity and pixel coordinates to 3D space.\n",
    "\n",
    "\n",
    "Refs: [1](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#ga617b1685d4059c6040827800e72ad2b6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb5f75-ef18-4b11-8229-111a4fec8a56",
   "metadata": {},
   "source": [
    "#### **1. Using `R1` or `R2`:**\n",
    "\n",
    "- **What You Have**: Points in the original, unrectified 2D image coordinates (e.g., $(u, v)$) for one or both cameras.\n",
    "- **How It's Used**:\n",
    "  - `R1` is part of the rectification process. It transforms 3D rays corresponding to the unrectified image points into a rectified coordinate system.\n",
    "  - Internally, the rectification process uses camera calibration parameters (intrinsic and extrinsic) to compute the rectified images. You typically don't directly apply `R1` unless you are manually transforming rays or doing advanced tasks like custom 3D geometry adjustments.\n",
    "\n",
    "- **Practical Use**:\n",
    "  - When stereo images are rectified, you typically get the rectified images as output. After that, the alignment of epipolar lines is already ensured.\n",
    "  - Once the images are rectified, you don't usually need to apply `R1` explicitly.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Using `P1` and `P2`:**\n",
    "\n",
    "- **What You Have**:\n",
    "  - 3D points in the rectified coordinate system (e.g., derived from disparity and `Q`).\n",
    "  - Or, you may have 2D image points in the rectified images and want to work backward to check their 3D projections.\n",
    "\n",
    "- **How It's Used**:\n",
    "  - `P1` projects rectified 3D points back to the 2D image plane for the first rectified image. Similarly, `P2` does this for the second rectified image.\n",
    "  - For stereo depth estimation, `P1` and `P2` are mostly used indirectly, as they help compute rectified disparity maps.\n",
    "  \n",
    "- **Practical Use**:\n",
    "  - If you're computing reprojection errors, for example, you might project estimated 3D points back into 2D images using `P1` or `P2` and compare with observed 2D points.\n",
    "  - If you are generating synthetic stereo pairs from a 3D model, you might use `P1` and `P2` to project the model's points into rectified images.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Using `Q`:**\n",
    "\n",
    "- **What You Have**:\n",
    "  - A disparity map, where each pixel's value represents the disparity $d$ between the corresponding points in the rectified images.\n",
    "  - $d = u_{\\text{left}} - u_{\\text{right}}$, where $u_{\\text{left}}$ and $u_{\\text{right}}$ are the horizontal coordinates of corresponding points in the rectified images.\n",
    "\n",
    "- **How It's Used**:\n",
    "  - `Q` is used to convert disparity into 3D points in the real-world coordinate system:\n",
    "    $\n",
    "    [X, Y, Z, W]^\\top = Q \\cdot [u, v, d, 1]^\\top\n",
    "    $\n",
    "    where:\n",
    "    - $u, v$: Pixel coordinates in the rectified image.\n",
    "    - $d$: Disparity value.\n",
    "    - $W$: Homogeneous scaling factor (typically $W = 1$).\n",
    "\n",
    "  - After the multiplication, divide by $W$ to get 3D coordinates:\n",
    "    $\n",
    "    X' = X/W, \\quad Y' = Y/W, \\quad Z' = Z/W\n",
    "    $\n",
    "\n",
    "- **Practical Use**:\n",
    "  - This is the core of depth estimation. `Q` allows you to take a disparity map and compute a 3D point cloud for the scene.\n",
    "  - For example, if you use `cv::reprojectImageTo3D`, OpenCV internally applies `Q` to compute the 3D points.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why Don't You Usually Use `R1`, `P1`, or `Q` Explicitly?**\n",
    "\n",
    "1. **Rectification (`R1` and `R2`)**:\n",
    "   - The rectification process is typically handled by functions like `cv::stereoRectify` or `cv::initUndistortRectifyMap`.\n",
    "   - After rectification, you work with rectified images directly, where epipolar geometry is simplified.\n",
    "\n",
    "2. **Projection Matrices (`P1` and `P2`)**:\n",
    "   - These are mostly useful for reprojection tasks (e.g., projecting 3D points back to 2D for validation).\n",
    "   - In most stereo vision workflows, you work with disparity maps and depth directly, not raw 3D projection.\n",
    "\n",
    "3. **Disparity-to-Depth Mapping (`Q`)**:\n",
    "   - `Q` is essential but is often used indirectly by functions like `cv::reprojectImageTo3D`, which compute 3D point clouds from disparity maps.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Summary of Typical Workflow**:\n",
    "\n",
    "1. **Calibration**: Compute camera parameters (intrinsic, extrinsic, and distortion coefficients).\n",
    "2. **Rectification**: Use `R1`, `R2`, `P1`, and `P2` to rectify images.\n",
    "3. **Disparity Estimation**: Compute a disparity map from the rectified stereo images.\n",
    "4. **3D Reconstruction**: Use `Q` with the disparity map to compute the 3D point cloud.\n",
    "\n",
    "This abstraction ensures that you rarely need to manually manipulate matrices like `R1`, `P1`, or `Q`. Instead, high-level functions handle these computations for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932a7e98-3aa6-4f20-bd68-0f8ddda38fa6",
   "metadata": {},
   "source": [
    "If cameras are not calibrated:\n",
    "```cpp\n",
    "cv::stereoRectifyUncalibrated\t(\tInputArray \tpoints1,\n",
    "InputArray \tpoints2,\n",
    "InputArray \tF,\n",
    "Size \timgSize,\n",
    "OutputArray \tH1,\n",
    "OutputArray \tH2,\n",
    "double \tthreshold = 5 \n",
    ")\t\t\n",
    "```\n",
    "\n",
    "Refs: [1](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#gaadc5b14471ddc004939471339294f052)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9000a16e-324f-477f-97db-755b8288aac2",
   "metadata": {},
   "source": [
    "\n",
    "#### **Disparity Definition:**\n",
    "Disparity $ d $ is given by the horizontal difference between the $ x $-coordinates of a corresponding point in the left and right images:\n",
    "$\n",
    "d = x - x'\n",
    "$\n",
    "Where:\n",
    "- $ x $ = x-coordinate of the point in the left image.\n",
    "- $ x'$ = x-coordinate of the corresponding point in the right image.\n",
    "\n",
    "Distance between the centers of the two camera lens is $BD = BC + CD$. \n",
    "\n",
    "The triangles are similar,\n",
    "\n",
    "- $ACB$ and $BFE$\n",
    "- $ACD$ and $DGH$\n",
    "\n",
    "${\\displaystyle {\\begin{aligned}{\\text{disparity=}}d&=EF+GH\\\\&=BF({\\frac {EF}{BF}}+{\\frac {GH}{BF}})\\\\&=BF({\\frac {EF}{BF}}+{\\frac {GH}{DG}})\\\\&=BF({\\frac {BC+CD}{AC}})\\\\&=BF{\\frac {BD}{AC}}\\\\&={\\frac {k}{z}}{\\text{, where}}\\\\\\end{aligned}}}$\n",
    "\n",
    "\n",
    "Where $k$ is the distance between the two cameras times the distance from the lens to the image.\n",
    "\n",
    "$k=BFBD$=$f \\times \\text{ Baseline}$\n",
    "\n",
    "${\\displaystyle d={\\frac {k}{Z}}}$\n",
    "\n",
    "${\\displaystyle d={\\frac {f \\times \\text{ Baseline}}{Z}}}$\n",
    "\n",
    "${\\displaystyle x-x'={\\frac {f \\times \\text{ Baseline}}{Z}}}$\n",
    "\n",
    "$Z = \\frac{f \\cdot B}{d=x-x'}  $\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/images_depth_to_displacement_relationship.png\" width=\"30%\" height=\"30%\" />\n",
    "<img src=\"images/stereo_depth.jpg\" width=\"30%\" height=\"30%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2244465-af20-45a7-b07f-df7b184dd5d1",
   "metadata": {},
   "source": [
    "```cpp\n",
    "void cv::triangulatePoints\t(\tInputArray \tprojMatr1,\n",
    "InputArray \tprojMatr2,\n",
    "InputArray \tprojPoints1,\n",
    "InputArray \tprojPoints2,\n",
    "OutputArray \tpoints4D \n",
    ")\t\n",
    "```\n",
    "\n",
    "Points are in homogeneous rectified coordinated, meaning in first (left) camera\n",
    "\n",
    "\n",
    "```\n",
    "  // Convert points from homogeneous to 3D (divide by w)\n",
    "  std::vector<cv::Point3f> triangulatedPoints;\n",
    "  for (int i = 0; i < points4D.cols; ++i) {\n",
    "    cv::Point3f point;\n",
    "    point.x = points4D.at<float>(0, i) / points4D.at<float>(3, i);\n",
    "    point.y = points4D.at<float>(1, i) / points4D.at<float>(3, i);\n",
    "    point.z = points4D.at<float>(2, i) / points4D.at<float>(3, i);\n",
    "    triangulatedPoints.push_back(point);\n",
    "  }\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Ref: [1](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#gad3fc9a0c82b08df034234979960b778c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b399cfa-8fe6-4d25-9ce2-b8d00c162ee7",
   "metadata": {},
   "source": [
    "# Undistort and RectifyMap\n",
    "\n",
    "```cpp\n",
    "initUndistortRectifyMap()\n",
    "void cv::initUndistortRectifyMap\t(\tInputArray \tcameraMatrix,\n",
    "InputArray \tdistCoeffs,\n",
    "InputArray \tR,\n",
    "InputArray \tnewCameraMatrix,\n",
    "Size \tsize,\n",
    "int \tm1type,\n",
    "OutputArray \tmap1,\n",
    "OutputArray \tmap2 \n",
    ")\t\n",
    "```\n",
    "\n",
    "The function computes the joint undistortion and rectification transformation and represents the result in the form of maps for remap. The undistorted image looks like original, **as if it is captured with a camera using the camera matrix =newCameraMatrix** \n",
    "and zero distortion. \n",
    "\n",
    "In case of a monocular camera, newCameraMatrix is usually equal to cameraMatrix, or it can be computed by getOptimalNewCameraMatrix for a better control over scaling. In case of a stereo camera, newCameraMatrix is normally set to P1 or P2 computed by stereoRectify .\n",
    "\n",
    "\n",
    "Refs: [1](https://docs.opencv.org/3.4/da/d54/group__imgproc__transform.html#ga7dfb72c9cf9780a347fbe3d1c47e5d5a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec5e2f-93c7-4e4d-91c0-802936030d61",
   "metadata": {},
   "source": [
    "# Stereo matcher\n",
    "\n",
    "```cpp\n",
    "static Ptr<StereoBM> cv::StereoBM::create(int numDisparities = 0, int blockSize = 21)\n",
    "```\n",
    "\n",
    "\n",
    "- `numDisparities`:\tthe disparity search range. For each pixel algorithm will find the best disparity from 0 (default minimum disparity) to numDisparities. \n",
    "\n",
    "- `blockSize`:\tthe linear size of the blocks, The size should be odd (as the block is centered at the current pixel). Larger block size implies smoother, though less accurate disparity map. Smaller block size gives more detailed disparity map, but there is higher chance for algorithm to find a wrong correspondence.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b2544-3704-4c38-b908-ccfdacbe3c17",
   "metadata": {},
   "source": [
    "# Disparity map\n",
    "\n",
    "```cpp\n",
    "cv::Mat disparity16S, disparity8U;\n",
    "stereoBM->compute(rectifiedImageLeft, rectifiedImageRight, disparity16S);\n",
    "```\n",
    "\n",
    "- By default, `StereoBM::compute` returns a 16-bit signed single-channel image (`CV_16S`), where each pixel stores the disparity in $\\frac{1}{16}$ of a pixel (i.e., the disparity is scaled by 16).\n",
    "- To visualize or post-process the disparity, you often convert it to an 8-bit image:\n",
    "\n",
    "Convert from 16S to 8U for easy visualization\n",
    "\n",
    "```cpp\n",
    "disparity16S.convertTo(disparity8U, CV_8U, 255.0/(numDisparities*16.0));\n",
    "```\n",
    "\n",
    "Now `disparity8U` is an 8-bit image you can display with `imshow` or save to disk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5391be-4797-4cd2-9eba-f3a224b7739d",
   "metadata": {},
   "source": [
    "<img src=\"images/disparity.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44f82c-3816-4ea2-a997-9976e655eedc",
   "metadata": {},
   "source": [
    "# Convert disparity to 3D (depth map)\n",
    "\n",
    "If you want a 3D point cloud or actual depth values, you can use `cv::reprojectImageTo3D` along with the \\(Q\\) matrix from `cv::stereoRectify`. When you call `cv::stereoRectify(...)`, one of the outputs is `Q`, which is a \\(4 \\times 4\\) reprojection matrix used to map disparity values to 3D coordinates.\n",
    "\n",
    "reprojectImageTo3D will compute `(X, Y, Z)` for each pixel\n",
    "\n",
    "\n",
    "```cpp\n",
    "cv::Mat xyz;  // Will hold 3D coordinates of each pixel\n",
    "cv::reprojectImageTo3D(disparity16S, xyz, Q, /* handleMissingValues = */ true);\n",
    "```\n",
    "\n",
    "The `xyz` matrix will be `CV_32FC3`, where each pixel contains \\((X, Y, Z)\\) in the **rectified camera’s coordinate system** (in whatever units your focal length/baseline imply).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2805cb-9873-4a1f-b8d0-229c04e666c1",
   "metadata": {},
   "source": [
    "# Distance Between the two cameras (Baseline) and stereo angle\n",
    "\n",
    "The short answer is: **it depends on your application.** Unlike human binocular vision—where eyes are fixed about 6–7 cm apart and slightly converged—engineering stereo setups can vary widely. That said, there are some useful rules of thumb and practical guidelines:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Baseline (Distance Between Cameras)\n",
    "\n",
    "- **Human-Sized Baseline (~6–7 cm).** If you aim to replicate human vision (e.g., VR or robotics that interacts with objects at roughly arm’s length to several meters), a baseline in this range is often ideal. It provides a reasonable balance between near and far depth perception.\n",
    "\n",
    "- **Wider Baseline (> 10 cm).**  \n",
    "  For detecting depth in environments where objects might be further away (e.g., several meters to tens of meters), a wider baseline increases disparity and yields more accurate depth at longer distances. However, if you’re looking at very close objects, a larger baseline can cause “dead zones” or excessive disparity.\n",
    "\n",
    "- **Narrow Baseline (< 5 cm).**  \n",
    "  Useful if you’re mostly interested in small, close objects or have physical constraints (like a compact camera rig). A narrower baseline reduces the maximum measurable depth but can improve matching accuracy for very close objects.\n",
    "\n",
    "**Rule of Thumb:**  \n",
    "Choose a baseline approximately **1/30 to 1/50 of the target distance** you care about. For example, if you’re consistently looking at objects around 1 meter away, consider a baseline of ~2–3 cm. If the objects are ~3 meters away, a 6–10 cm baseline might be better, and so on.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Stereo Angle (Toe-In vs. Parallel)\n",
    "\n",
    "1. **Parallel Cameras (0° Stereo Angle)**  \n",
    "   - Easiest calibration and rectification (the image planes align more simply in software).  \n",
    "   - Common choice in many stereo vision setups.  \n",
    "   - You rely on epipolar geometry in post-processing to “shift” and find matching features.  \n",
    "\n",
    "2. **Slight Toe-In (Convergence Angle ~5–10°)**  \n",
    "   - Mimics the human eye convergence more closely.  \n",
    "   - Can improve depth measurement for a known region of interest (e.g., a near focus point).  \n",
    "   - More complex geometry: you must ensure accurate calibration since each camera has a different perspective with a non-zero yaw angle.\n",
    "\n",
    "**Rule of Thumb:**  \n",
    "- If you have a single, well-defined depth region (e.g., an object on a conveyor belt always at 1 m), you can angle your cameras slightly inward to center that region in your sensors and maximize disparity where it matters.  \n",
    "- If your depth range varies a lot, parallel mounting keeps it simpler and is easier to generalize.\n",
    "\n",
    "---\n",
    "\n",
    "#### Putting It All Together\n",
    "\n",
    "1. **Determine Your Primary Depth Range**  \n",
    "   - How close/far are the objects you want to measure?\n",
    "\n",
    "2. **Select a Baseline**  \n",
    "   - Use a baseline that yields good disparity over that range (e.g., 6–7 cm if it’s roughly human-scale distances, or larger if you need more depth coverage).\n",
    "\n",
    "3. **Decide on Stereo Angle**  \n",
    "   - **Parallel** for general-purpose or wide-range stereo.  \n",
    "   - **Slight convergence** for a more controlled, narrow-depth application.  \n",
    "\n",
    "4. **Calibrate Carefully**  \n",
    "   - Stereo calibration (finding intrinsic and extrinsic parameters) is critical no matter the baseline or angle. Good calibration ensures accurate depth reconstruction.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Scenarios\n",
    "\n",
    "1. **Mobile Robot or Drone**  \n",
    "   - Often uses a baseline close to 6–10 cm (human-like) for obstacle avoidance at 1–5 m range.  \n",
    "   - Cameras are usually mounted parallel for simpler algorithms.\n",
    "\n",
    "2. **Industrial 3D Inspection**  \n",
    "   - Baselines can be larger (10–30 cm or more) if objects are 1–5 meters away.  \n",
    "   - Might slightly toe-in the cameras to optimize for a conveyor belt’s known path.\n",
    "\n",
    "3. **VR/AR Headset**  \n",
    "   - Typically ~6.3–6.5 cm to match the average human inter-pupillary distance (IPD).  \n",
    "   - Slight toe-in might replicate natural eye convergence; often the software pipeline handles stereo rendering in parallel mode for convenience.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Takeaways\n",
    "\n",
    "- **Baseline** is your primary lever for tuning the stereo system to a specific depth range.  \n",
    "- **Small baseline** = better for short-range precision; **large baseline** = better for long-range resolution.  \n",
    "- **Stereo angle** can be parallel or slightly toed-in; parallel is simpler but toe-in can be useful for a narrower focus range.  \n",
    "\n",
    "By balancing these two factors and performing a precise stereo calibration, you can optimize your stereo camera setup for your particular application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f8bc0d-0482-4e8b-b56d-1539cb5ded89",
   "metadata": {},
   "source": [
    "Refs: [1](https://towardsdatascience.com/a-comprehensive-tutorial-on-stereo-geometry-and-stereo-rectification-with-python-7f368b09924a)\n",
    "\n",
    "[Python code](../scripts/multi_snapshot_stereo.py)\n",
    "\n",
    "\n",
    "[C++ code](../src/virtual_stereo_vision_cameras.cpp)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
